{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Trabajo práctico 7-ExequielSantucho","provenance":[{"file_id":"1pLVG-94qieVpzkikp7fJDlWmrwGHAqok","timestamp":1622817815321},{"file_id":"1KEv72HobydbF6E5GIIKwbusSnbCwo5xd","timestamp":1622129629201}],"collapsed_sections":["bJTGmc5TE2O4"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"t3p9-fDtzWaA"},"source":["#@title Aprendizaje Profundo | Otoño 2021 by Datitos{display-mode: \"form\" }\n","#@markdown ![71335171.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAIAAAAiOjnJAAACwElEQVR4nOzdMY7iQBBA0WU197/FnJNNJ/FqWvLHZfd7McIGfVVQos3X+/3+A2f7e/UN8EzCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBJfV9/A/7xer6XX3/1/gZ70eU0sEsIiISwSwiIhLBLCIiEsEiP2WEf7m9U9zVnvU9vh85pYJIRFQlgkhEVCWCSERUJYJEbssc5ytL+5at8zec9UM7FICIuEsEgIi4SwSAiLhLBIPGqPdWR1v1VfdwcmFglhkRAWCWGREBYJYZEQFokt9lhHdt4z1UwsEsIiISwSwiIhLBLCIiEsEh/dY+18zq4w7RzlTyYWCWGREBYJYZEQFglhkRAWiS1+jzVhr/Mbd7nP3zCxSAiLhLBICIuEsEgIi4SwSDxqj1U/72r1ulc9l2sCE4uEsEgIi4SwSAiLhLBICIvE7D3W9/fSy6/63dLqdZfvc/F7mMDEIiEsEsIiISwSwiIhLBLCIjF7j3WSs87rTXufyUwsEsIiISwSwiIhLBLCIiEsEq8Ju5N6r3OXc3xP2oeZWCSERUJYJIRFQlgkhEVCWCRG7LGOTN7TfNIdvwcTi4SwSAiLhLBICIuEsEgIi8Toc4U7Pyf9p8n7qiMmFglhkRAWCWGREBYJYZEQFonRe6wjd9zr7MbEIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIi8S8AAP//HtRtH09JwIEAAAAASUVORK5CYII=)\n","#El siguiente notebook fue traducido por Pablo Marinozi como el séptimo trabajo práctico correspondiente a la versión de Otoño del 2021 del curso Aprendizaje Profundo organizado por Datitos\n","#El tutorial original fue diseñado por Ben Trevett y fue publicado en su github https://github.com/bentrevett\n","#Para mayor información consultar https://datitos.github.io/curso-aprendizaje-profundo/#calendario"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLiZyyaIzk71"},"source":["#Trabajo Práctico N° 7: Aprendizaje Secuencia a Secuencia con Redes Neuronales"]},{"cell_type":"markdown","metadata":{"id":"htb8PY4M33cF"},"source":["# Sección 1 - Encoder-Decoder con RNNs\n","\n","En este práctico, crearemos un modelo de aprendizaje automático para pasar de una secuencia a otra, utilizando PyTorch y torchtext. Esto se hará en traducciones del alemán al inglés, pero los modelos se pueden aplicar a cualquier problema que implique pasar de una secuencia a otra, como el resumen, es decir, pasar de una secuencia a una secuencia más corta en el mismo idioma.\n","\n","En este primer notebook, comenzaremos a comprender los conceptos generales de manera simple mediante la implementación del modelo del documento [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215).\n","\n","## Introducción\n","\n","Los modelos de secuencia a secuencia (seq2seq) más comunes son los modelos *encoder-decoder*, que comúnmente usan una *red neuronal recurrente* (RNN) para *codificar* la oración de origen (entrada) en un solo vector. En este notebook, nos referiremos a este vector único como *vector de contexto*. Podemos pensar en el vector de contexto como una representación abstracta de toda la oración de entrada. Este vector es luego *decodificado* por un segundo RNN que aprende a generar la oración de destino (de salida) generándola una palabra a la vez.\n","\n","![](https://i.imgur.com/HZPIbyC.png)\n","\n","La imagen de arriba muestra un ejemplo de traducción. La oración de entrada/fuente, \"guten morgen\", se pasa a través de la capa de embedding (amarillo) y luego se ingresa en el encoder (verde). También agregamos un símbolo de *inicio de secuencia* (`<sos>`) y *fin de secuencia* (`<eos>`) al inicio y al final de la oración, respectivamente. En cada paso de tiempo, la entrada al encoder RNN es tanto el embedding, $e$, de la palabra actual, $e(x_t)$, como el estado oculto del paso de tiempo anterior, $ h_{t -1} $, y el codificador RNN genera un nuevo estado oculto $ h_t $. Podemos pensar en el estado oculto como una representación vectorial de la oración hasta ahora. El RNN se puede representar como una función de $ e(x_t) $ y $ h_{t-1} $:\n","\n","$$h_t = \\text{EncoderRNN}(e(x_t), h_{t-1})$$\n","\n","Usamos el término RNN generalmente aquí, podría ser cualquier arquitectura recurrente, como una *LSTM* (Long Short-Term Memory) o una *GRU* (Gated Recurrent Unit).\n","\n","Aquí, tenemos $ X = \\{x_1, x_2, ..., x_T \\} $, donde $ x_1 = \\text{<sos>}, x_2 = \\text{guten} $, etc. El estado oculto inicial, $ h_0 $, generalmente se inicializa en ceros o en un parámetro aprendido.\n","\n","Una vez que la última palabra, $ x_T $, se ha pasado al RNN a través de la capa de embedding, usamos el estado oculto final, $ h_T $, como vector de contexto, es decir, $ h_T = z $. Esta es una representación vectorial de toda la oración fuente.\n","\n","Ahora que tenemos nuestro vector de contexto, $ z $, podemos comenzar a decodificarlo para obtener la oración de salida/objetivo, \"good morning\". Nuevamente, agregamos tokens de inicio y final de secuencia a la oración de destino. En cada paso de tiempo, la entrada al decoder RNN (azul) es el embedding, $ d $, de la palabra actual, $ d(y_t) $, así como el estado oculto del paso de tiempo anterior, $ s_{ t-1} $, donde el estado oculto inicial del decoder , $ s_0 $, es el vector de contexto, $ s_0 = z = h_T $, es decir, el estado oculto inicial del decoder es el estado oculto final del encoder. Por lo tanto, de manera similar al encoder, podemos representar el decoder como:\n","\n","$$s_t = \\text{DecoderRNN}(d(y_t), s_{t-1})$$\n","\n","Aunque la capa de embedding de entrada/origen, $ e $, y la capa de embedding de salida/destino, $ d $, se muestran en amarillo en el diagrama, son dos capas de embedding diferentes con sus propios parámetros.\n","\n","En el decoder, necesitamos pasar del estado oculto a una palabra real, por lo tanto, en cada paso de tiempo usamos $ s_t $ para predecir (pasándolo a través de una capa `Densa`, que se muestra en violeta) la que pensamos que es la siguiente palabra en la secuencia, $ \\hat{y}_t $.\n","\n","$$\\hat{y}_t = f(s_t)$$\n","\n","Las palabras en el decoder siempre se generan una tras otra, con una por paso de tiempo. Siempre usamos `<sos>` para la primera entrada al decoder, $ y_1 $, pero para las entradas subsiguientes, $ y_{t> 1} $, a veces usaremos la siguiente palabra de la secuencia ground truth, $ y_t $ y a veces usamos la palabra predicha por nuestro decoder, $ \\hat{y}_{t-1} $. Esto se llama *forzamiento del maestro*, vea un poco más de información al respecto [aquí](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/).\n","\n","Al entrenar/evaluar nuestro modelo, siempre sabemos cuántas palabras hay en nuestra oración objetivo, por lo que dejamos de generar palabras una vez que alcanzamos esa cantidad. Durante la inferencia, es común seguir generando palabras hasta que el modelo genera un token `<eos>` o después de que se haya generado una cierta cantidad de palabras.\n","\n","Una vez que tenemos nuestra oración objetivo predicha, $ \\hat{Y} = \\{\\hat{y} _1, \\hat{y} _2, ..., \\hat{y}_T \\} $, la comparamos con nuestra oración objetivo real, $ Y = \\{y_1, y_2, ..., y_T \\} $, para calcular nuestra pérdida. Luego usamos esta pérdida para actualizar todos los parámetros en nuestro modelo.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BhDoLfA3woba"},"source":["## Preparando los datos\n","\n","Vamos a programar los modelos en PyTorch y usar torchtext para ayudarnos a hacer todo el preprocesamiento requerido. También usaremos spaCy para ayudar en la tokenización de los datos."]},{"cell_type":"code","metadata":{"id":"cHOjdy-S33cN"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F # Agrego esto que no estaba\n","\n","from torchtext.legacy.datasets import Multi30k\n","from torchtext.legacy.data import Field, BucketIterator, Example, Dataset\n","\n","import spacy\n","import numpy as np\n","\n","import random\n","import math\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CtDp0fVb33cQ"},"source":["Vamos a establecer las semillas aleatorias para obtener resultados deterministas."]},{"cell_type":"code","metadata":{"id":"b4qIfknf33cR"},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBUjPOwv33cS"},"source":["A continuación, vamos a crear los tokenizadores. Un tokenizador se usa para convertir una cadena que contiene una oración en una lista de tokens individuales que componen esa cadena, p. Ej. \"Good morning!\" se convierte en [\"good\", \"morning\", \"!\"]. Empezaremos a tratar las oraciones como una secuencia de tokens a partir de ahora, en lugar de decir que son una secuencia de palabras. \n","\n","spaCy tiene un modelo para cada idioma (\"de_core_news_sm\" para alemán y \"en_core_web_sm\" para inglés) que debe cargarse para poder acceder al tokenizador de cada modelo.\n","\n","**Nota**: descarga los modelos de spaCy ejecutando la siguiente celda\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"zSpToClFQ2Jm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623102397543,"user_tz":180,"elapsed":6755,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"ee19cc8c-a7d6-4b83-f37f-8804671f39b9"},"source":["!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_sm')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_txKXYl2xb_W"},"source":["Después de descargar los modelos, tenemos que reiniciar el entorno de ejecución y correr la siguiente celda."]},{"cell_type":"code","metadata":{"id":"XEFvMbCB33cU"},"source":["spacy_de = spacy.load('de_core_news_sm')\n","spacy_en = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vbv_e1x_33cV"},"source":["A continuación, vamos a crear las funciones del tokenizador. Estas se pueden pasar a torchtext y transformarán la oración en una lista de tokens.\n","\n","En el paper que estamos implementando, les resulta beneficioso invertir el orden de la entrada porque creen que \"introduce muchas dependencias a corto plazo en los datos que facilitan mucho el problema de optimización\". Copiamos esto invirtiendo la oración en alemán después de que se haya transformado en una lista de tokens.\n"]},{"cell_type":"code","metadata":{"id":"ViAi0_Ay33cW"},"source":["def tokenize_de(text):\n","    \"\"\"\n","    Tokeniza el texto en alemán de una cadena a una lista de cadenas (tokens) y lo invierte\n","    \"\"\"\n","    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n","\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokeniza el texto en inglés de una cadena a una lista de cadenas (tokens)\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vzldNzrU33cW"},"source":["Los `Field`s de torchtext manejan cómo se deben procesar los datos. Todos los argumentos posibles se detallan [aquí](https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L61).\n","\n","Seteamos el argumento `tokenize` en la función de tokenización correcta para cada uno, siendo el alemán el Field ` SRC` (origen) y el inglés el Field `TRG` (destino). El Field también agrega los tokens de \"inicio de secuencia\" y \"fin de secuencia\" a través de los argumentos `init_token` y` eos_token`, y convierte todas las palabras a minúsculas."]},{"cell_type":"code","metadata":{"id":"D2FJ18Ss33cX"},"source":["SRC = Field(tokenize = tokenize_de, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","\n","TRG = Field(tokenize = tokenize_en, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asWSq4-N33cZ"},"source":["A continuación, descargamos y cargamos los datasets de entrenamiento, validación y prueba.\n","\n","El dataset que usaremos es el [Multi30k](https://github.com/multi30k/dataset). Este es un conjunto de datos con ~30,000 oraciones paralelas en inglés, alemán y francés, cada una con ~12 palabras por oración.\n","\n","`exts` especifica qué idiomas usar como origen y destino (el origen va primero) y ` fields` especifica qué Fields usar para cada uno."]},{"cell_type":"code","metadata":{"id":"D7u8dUiB33ca","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"status":"error","timestamp":1623154353355,"user_tz":180,"elapsed":5025,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"69ad9768-c38b-4f52-c5da-63eb828fb623"},"source":["train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n","                                                    fields = (SRC, TRG))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["downloading training.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 1.04MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["downloading validation.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 158kB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["downloading mmt_task1_test2016.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 154kB/s]\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-de17d0121e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n\u001b[0;32m----> 2\u001b[0;31m                                                     fields = (SRC, TRG))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/datasets/translation.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, exts, fields, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         return super(Multi30k, cls).splits(\n\u001b[0;32m--> 114\u001b[0;31m             exts, fields, path, root, train, validation, test, **kwargs)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/datasets/translation.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, exts, fields, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         train_data = None if train is None else cls(\n\u001b[0;32m---> 66\u001b[0;31m             os.path.join(path, train), exts, fields, **kwargs)\n\u001b[0m\u001b[1;32m     67\u001b[0m         val_data = None if validation is None else cls(\n\u001b[1;32m     68\u001b[0m             os.path.join(path, validation), exts, fields, **kwargs)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/datasets/translation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, exts, fields, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msrc_line\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     examples.append(data.Example.fromlist(\n\u001b[0;32m---> 40\u001b[0;31m                         [src_line, trg_line], fields))\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTranslationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/example.py\u001b[0m in \u001b[0;36mfromlist\u001b[0;34m(cls, data, fields)\u001b[0m\n\u001b[1;32m     82\u001b[0m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    207\u001b[0m         `preprocessing` Pipeline.\"\"\"\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-63fa583bb861>\u001b[0m in \u001b[0;36mtokenize_de\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTokeniza\u001b[0m \u001b[0mel\u001b[0m \u001b[0mtexto\u001b[0m \u001b[0men\u001b[0m \u001b[0malemán\u001b[0m \u001b[0mde\u001b[0m \u001b[0muna\u001b[0m \u001b[0mcadena\u001b[0m \u001b[0ma\u001b[0m \u001b[0muna\u001b[0m \u001b[0mlista\u001b[0m \u001b[0mde\u001b[0m \u001b[0mcadenas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0my\u001b[0m \u001b[0mlo\u001b[0m \u001b[0minvierte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy_de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'spacy_de' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"q8okwFR6ENFW"},"source":["Esta etapa de tokenización puede tardar alrededor de 20 minutos en ejecutarse. Para evitar tener que esperar tanto tiempo cada vez que generemos una sesión en Colab, vamos a guardar el dataset tokenizado en archivos.\n","\n","**Nota: Los archivos se borrarán cuando se desconecte la sesión. Asegurate de descargarlos a tu máquina o guardarlos en tu drive.** "]},{"cell_type":"code","metadata":{"id":"vLsOzKgcMWJt"},"source":["import json\n","def save_examples(dataset, savepath):\n","    with open(savepath, 'w') as f:\n","        # Save num. elements (not really need it)\n","        f.write(json.dumps(len(dataset.examples)))  # Write examples length\n","        f.write(\"\\n\")\n","\n","        # Save elements\n","        for pair in dataset.examples:\n","            data = [pair.src, pair.trg]\n","            f.write(json.dumps(data))  # Write samples\n","            f.write(\"\\n\")\n","\n","\n","def load_examples(filename):\n","    examples = []\n","    with open(filename, 'r') as f:\n","        # Read num. elements (not really need it)\n","        total = json.loads(f.readline())\n","\n","        # Save elements\n","        for i in range(total):\n","            line = f.readline()\n","            example = json.loads(line)\n","            # example = data.Example().fromlist(example, fields)  # Create Example obj. (you can do it here or later)\n","            examples.append(example)\n","    return examples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zW-qMhTbMamd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623154279691,"user_tz":180,"elapsed":23498,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"3b1fa438-1414-4e8a-da29-c0bad7fee53f"},"source":["#Guarda los datasets tokenizados en disco # No uso esto\n","# save_examples(train_data,\"train.json\")\n","# save_examples(valid_data,\"valid.json\")\n","# save_examples(test_data,\"test.json\")\n","\n","# ---\n","# Guardo los datasets tokenizados en carpeta de Google Drive\n","# Monto Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# Al continuar trabajo en otro día o si se finaliza el entorno de ejecución,\n","# ejecutar esta celda y la siguiente. Si tira error, ejecutar las anteriores\n","# para cargar las variables que no estén."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kX7d2gKGzdoS","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"error","timestamp":1623154374809,"user_tz":180,"elapsed":355,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"6f11854b-8ef0-4b31-c81a-6bfc29d08431"},"source":["save_examples(train_data,\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/train.json\")\n","save_examples(valid_data,\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/valid.json\")\n","save_examples(test_data,\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/test.json\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-98bbc3640486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/train.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/valid.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/test.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"5HRscUrHKhA3"},"source":["#Carga los datasets tokenizados desde los archivos\n","SRC = Field(tokenize = tokenize_de, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","\n","TRG = Field(tokenize = tokenize_en, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","fields = [('src', SRC), ('trg', TRG)]\n","\n","# examples = load_examples(\"train.json\") # cargo desde Google Drive\n","examples = load_examples(\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/train.json\")\n","examples = [Example().fromlist(d, fields) for d in examples]\n","train_data = Dataset(examples, fields)\n","\n","# examples = load_examples(\"valid.json\") # cargo desde Google Drive\n","examples = load_examples(\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/valid.json\")\n","examples = [Example().fromlist(d, fields) for d in examples]\n","valid_data = Dataset(examples, fields)\n","\n","# examples = load_examples(\"test.json\") # cargo desde Google Drive\n","examples = load_examples(\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP7/tmp/test.json\")\n","examples = [Example().fromlist(d, fields) for d in examples]\n","test_data = Dataset(examples, fields)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCOorTh_33ca"},"source":["Podemos verificar que hayamos cargado la cantidad correcta de ejemplos:\n","\n","*   Elemento de lista\n","*   Elemento de lista\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjWessnm33cc","executionInfo":{"status":"ok","timestamp":1623154391978,"user_tz":180,"elapsed":314,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"0d3e44ae-a8df-453c-e53a-3900c814a7e1"},"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 29000\n","Number of validation examples: 1014\n","Number of testing examples: 1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wLC4ggJv33cc"},"source":["También podemos imprimir un ejemplo, asegurándonos de que la oración original esté invertida:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEYyQgEA33cd","executionInfo":{"status":"ok","timestamp":1623154395088,"user_tz":180,"elapsed":321,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"aca3f903-f99f-46c9-dc83-e37e25a43a1d"},"source":["print(vars(train_data.examples[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c7johCL633ce"},"source":["El punto está al principio de la oración en alemán, por lo que parece que la oración se ha invertido correctamente.\n","\n","A continuación, vamos a armar el *vocabulario* para los idiomas de origen y destino. El vocabulario se usa para asociar cada token único con un índice (un número entero). Los vocabularios de los idiomas de origen y de destino son distintos.\n","\n","Usando el argumento `min_freq`, solo permitimos que los tokens que aparecen al menos 2 veces aparezcan en nuestro vocabulario. Los tokens que aparecen solo una vez se convierten en un token `<unk>` (desconocido).\n","\n","Es importante tener en cuenta que nuestro vocabulario solo debe construirse a partir del conjunto de entrenamiento y no del conjunto de validación/prueba. Esto evita la \"fuga de información\" en nuestro modelo, dándonos puntuaciones de validación/prueba artificialmente infladas."]},{"cell_type":"code","metadata":{"id":"6TEFpr7b33cf"},"source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qms-GDr733cf","executionInfo":{"status":"ok","timestamp":1623154401450,"user_tz":180,"elapsed":18,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"f51ba4b0-11f4-41b8-b9e0-73cec60be808"},"source":["print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (de) vocabulary: 7855\n","Unique tokens in target (en) vocabulary: 5893\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XPkb6DDC33cg"},"source":["El paso final de la preparación de los datos es crear los iteradores. Estos se pueden iterar para devolver un lote de datos que tendrá un atributo `src` (los tensores PyTorch que contienen un lote de oraciones en el idioma origen numericalizadas) y un atributo` trg` (los tensores PyTorch que contienen un lote de oraciones en el idioma objetivo numericalizadas). \"Numericalizada\" es solo una forma elegante de decir que se han convertido de una secuencia de tokens legibles a una secuencia de índices correspondientes, usando el vocabulario.\n","\n","También necesitamos definir un `dispositivo`. Esto se usa para decirle a torchText que ponga los tensores en la GPU o no. Usamos la función `torch.cuda.is_available()`, que devolverá `True` si se detecta una GPU en nuestra computadora. Pasamos este \"dispositivo\" al iterador.\n","\n","Cuando obtenemos un lote de ejemplos usando un iterador, debemos asegurarnos de que todas las oraciones fuente se rellenen con la misma longitud, lo mismo con las oraciones objetivo. ¡Afortunadamente, los iteradores de torchText manejan esto por nosotros!\n","\n","Usamos un `BucketIterator` en lugar del` Iterator` estándar ya que crea lotes de tal manera que minimiza la cantidad de relleno en las oraciones de origen y de destino."]},{"cell_type":"code","metadata":{"id":"f_rWjr0_33ch"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fC2siLK033ci"},"source":["BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort=False, \n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3g-NB7vWUwdi"},"source":["## Construyendo el modelo Seq2Seq\n","\n","Construiremos nuestro modelo en tres partes. El encoder, el decoder y un modelo seq2seq que los encapsula a ambos y proporcionará una forma de interactuar con cada uno."]},{"cell_type":"markdown","metadata":{"id":"SR8ujsCW33ck"},"source":["### Encoder\n","\n","Primero, el encoder es un LSTM de 2 capas. El paper que estamos implementando utiliza un LSTM de 4 capas, pero en aras de disminuir el tiempo de entrenamiento, lo redujimos a 2 capas. El concepto de RNN multicapa es fácil de expandir de 2 a 4 capas.\n","\n","Para un RNN multicapa, la oración de entrada, $ X $, después de pasar por una capa de embedding va a la primera capa (la inferior) del RNN y los estados ocultos, $ H = \\{h_1, h_2, ..., h_T \\} $ , producidos por esta capa se utilizan como entradas para el RNN en la capa superior. Por lo tanto, al representar cada capa con un superíndice, los estados ocultos en la primera capa vienen dados por:\n","\n","$$h_t^1 = \\text{EncoderRNN}^1(e(x_t), h_{t-1}^1)$$\n","\n","Los estados ocultos en la segunda capa vienen dados por:\n","\n","$$h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$$\n","\n","El uso de un RNN multicapa también significa que necesitaremos un estado oculto inicial como entrada por capa, $ h_0 ^ l $, y también generaremos un vector de contexto por capa, $ z ^ l $.\n","\n","Sin entrar en demasiados detalles sobre las LSTM (ver [esto](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) para obtener más información sobre ellas), todo lo que necesitamos saber es que son un tipo de RNN que en lugar de simplemente tomar un estado oculto y devolver un nuevo estado oculto por paso de tiempo, también toma y devuelve un *estado de celda*, $ c_t $, por paso de tiempo.\n","\n","$$\\begin{align*}\n","h_t &= \\text{RNN}(e(x_t), h_{t-1})\\\\\n","(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\n","\\end{align*}$$\n","\n","Podemos pensar en $ c_t $ como otro tipo de estado oculto. Similar a $ h_0 ^ l $, $ c_0 ^ l $ se inicializará a un tensor de todos ceros. Además, nuestro vector de contexto ahora será tanto el estado oculto final como el estado final de la celda, es decir, $ z ^ l = (h_T ^ l, c_T ^ l) $.\n","\n","Al extender nuestras ecuaciones multicapa a LSTM, obtenemos:\n","\n","$$\\begin{align*}\n","(h_t^1, c_t^1) &= \\text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))\\\\\n","(h_t^2, c_t^2) &= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n","\\end{align*}$$\n","\n","Observe cómo solo nuestro estado oculto de la primera capa se pasa como entrada a la segunda capa, y no el estado de la celda.\n","\n","Entonces nuestro encoder se parece a esto:\n","\n","![](https://i.imgur.com/4s0fceB.png)\n","\n","Creamos esto en código haciendo un módulo `Encoder`, que requiere que heredemos de `torch.nn.Module` y usemos `super().__ init __()` como código repetitivo. El encoder toma los siguientes argumentos:\n","- `input_dim` es el tamaño/dimensionalidad de los vectores one-hot que se ingresarán al encoder. Esto es igual al tamaño del vocabulario de entrada (origen).\n","- `emb_dim` es la dimensionalidad de la capa de embedding. Esta capa convierte los vectores one-hot en vectores densos con dimensión `emb_dim`.\n","- `hid_dim` es la dimensionalidad de los estados ocultos y de celda.\n","- `n_layers` es el número de capas en el RNN.\n","- `dropout` es la probabilidad de dropout que se debe utilizar. Este es un parámetro de regularización para evitar el sobreajuste. Consulte [esto](https://www.coursera.org/lecture/deep-neural-network/understanding-dropout-YaGbR) para obtener más detalles sobre el dropout.\n","\n","No vamos a discutir la capa de embedding en detalle durante estos tutoriales. Todo lo que necesitamos saber es que hay un paso antes de que las palabras (técnicamente, los índices de las palabras) se pasen al RNN, donde las palabras se transforman en vectores. Para leer más sobre embedding de palabras, consulte estos artículos: [1](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/), [2](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html), [3](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ ), [4](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/).\n","\n","La capa de embedding se crea usando `nn.Embedding`, la LSTM con `nn.LSTM` y la capa de dropout con `nn.Dropout`. Consulte la [documentación](https://pytorch.org/docs/stable/nn.html) de PyTorch para obtener más información sobre estas.\n","\n","Una cosa a tener en cuenta es que el argumento \"dropout\" del LSTM es cuánto dropout aplicar entre las capas de un RNN multicapa, es decir, entre la salida de estados ocultos de la capa $ l $ y esos mismos estados ocultos que se utilizan para la entrada de la capa $ l + 1 $.\n","\n","En el método `forward`, pasamos la oración de origen, $ X $, que se convierte en vectores densos usando la capa` embedding`, y luego se aplica el dropout. Estos embeddings luego se pasan al RNN. A medida que pasamos una secuencia completa al RNN, ¡automáticamente hará el cálculo recurrente de los estados ocultos en toda la secuencia por nosotros! Tenga en cuenta que no pasamos un estado inicial oculto o de celda al RNN. Esto se debe a que, como se indica en la [documentación](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM), si no se pasa ningún estado oculto/de celda al RNN, se crea automáticamente un estado oculto/de celda inicial como un tensor de todos ceros.\n","\n","El RNN devuelve: `outputs` (el estado oculto de la capa superior para cada paso de tiempo), `hidden` (el estado oculto final de cada capa, $ h_T $, apilados uno encima del otro) y `cell` (el estado de celda final para cada capa, $ c_T $, apilados uno encima del otro).\n","\n","Como solo necesitamos los estados finales ocultos y de celda (para hacer nuestro vector de contexto), `forward` solo devuelve `hidden` y `cell`.\n","\n","Los tamaños de cada uno de los tensores se dejan como comentarios en el código. En esta implementación, `n_directions` siempre será 1, sin embargo, tenga en cuenta que las RNN bidireccionales (usada en el TP 6) tendrán` n_directions` igual a 2."]},{"cell_type":"code","metadata":{"id":"KFZXw6Tc33cl"},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el encoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.dropout = nn.Dropout(dropout)\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)        \n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, src):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el encoder. Deberías        #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        \n","        #src = [src len, batch size]\n","        embedded = self.embedding(src)\n","        embedded = self.dropout(embedded)\n","        \n","        #embedded = [src len, batch size, emb dim]\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","\n","        #outputs = [src len, batch size, hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        #cell = [n layers * n directions, batch size, hid dim]\n","        # pass\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","        return hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NFo4RAu033cm"},"source":["### Decoder\n","\n","A continuación, construiremos nuestro decoder, que también será un LSTM de 2 capas (4 en el paper).\n","\n","![](https://i.imgur.com/aTf2uPT.png)\n","\n","La clase `Decoder` realiza un solo paso de decodificación, es decir, genera un solo token por paso de tiempo. La primera capa recibirá un estado oculto y de celda del paso de tiempo anterior, $ (s_{t-1} ^ 1, c_{t-1} ^ 1) $, y lo alimenta a través del LSTM con el embedding del token actual , $ y_t $, para producir un nuevo estado oculto y de celda, $ (s_t ^ 1, c_t ^ 1) $. Las capas siguientes utilizarán el estado oculto de la capa inferior, $ s_t ^ {l-1} $, y los estados ocultos y de celda anteriores de su misma capa, $ (s_{t-1} ^ l, c_{t-1 } ^ l) $. Esto proporciona ecuaciones muy similares a las del encoder.\n","\n","$$\\begin{align*}\n","(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\\\\n","(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n","\\end{align*}$$\n","\n","Recuerde que los estados ocultos y de celda iniciales de nuestro decoder son nuestros vectores de contexto, que son los estados ocultos y de celda finales de nuestro encoder de la misma capa, es decir, $ (s_0 ^ l, c_0 ^ l) = z ^ l = (h_T ^ l, c_T ^ l) $.\n","\n","Luego pasamos el estado oculto de la capa superior del RNN, $ s_t ^ L $, a través de una capa lineal, $ f $, para hacer una predicción de cuál debería ser el siguiente token en la secuencia objetivo (salida), $ \\hat{y}_{t + 1} $. \n","\n","$$\\hat{y}_{t+1} = f(s_t^L)$$\n","\n","Los argumentos y la inicialización son similares a la clase `Encoder`, excepto que ahora tenemos un `output_dim` que es el tamaño del vocabulario para la salida/destino. También hay una adición de la capa `Linear`, que se utiliza para hacer las predicciones desde el estado oculto de la capa superior.\n","\n","Dentro del método `forward`, aceptamos un lote de tokens de entrada, estados ocultos anteriores y estados de celda anteriores. Como solo decodificamos un token a la vez, los tokens de entrada siempre tendrán una longitud de secuencia de 1. Hacemos un `unsqueeze` sobre los tokens de entrada para agregar una dimensión de longitud de oración de 1. Luego, de manera similar al encoder, pasamos a través de un capa de embedding y aplicamos dropout. Este lote de embeddings de tokens se pasa luego al RNN con los estados ocultos y de celda anteriores. Esto produce un tensor `output` (estado oculto de la capa superior del RNN), un nuevo tensor de estados ocultos  `hidden`  (uno para cada capa, apilados uno encima del otro) y un nuevo tensor de estados de celda `cell`  (también uno por capa, apilados uno encima del otro). Luego pasamos `output` (después de deshacernos de la dimensión de longitud de la oración) a través de la capa densa para obtener nuestra `prediction`. Luego devolvemos la `prediction`, el nuevo estado oculto `hidden` y el nuevo estado de la celda `cell`.\n"]},{"cell_type":"code","metadata":{"id":"dBQRsYJB33cm"},"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el decoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","        \n","        super().__init__()\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.output_dim = output_dim\n","        self.dropout = nn.Dropout(dropout)\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n","        self.fc = nn.Linear(hid_dim, output_dim)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, input, hidden, cell):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el decoder. Deberías        #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        \n","        #input = [batch size]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        #cell = [n layers * n directions, batch size, hid dim]\n","        \n","        #n directions en el decoder será siempre 1, por lo tanto:\n","        #hidden = [n layers, batch size, hid dim]\n","        #cell = [n layers, batch size, hid dim]\n","        \n","        input = input.unsqueeze(0)\n","        #input = [1, batch size]       \n","\n","        embedded = self.embedding(input)\n","        embedded = self.dropout(embedded)\n","        #embedded = [1, batch size, emb dim]\n","\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        #output = [seq len, batch size, hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        #cell = [n layers * n directions, batch size, hid dim]\n","        \n","        #seq len y n directions en el decoder serán siempre 1, por lo tanto:\n","        #output = [1, batch size, hid dim]\n","        #hidden = [n layers, batch size, hid dim]\n","        #cell = [n layers, batch size, hid dim]\n","        \n","        output = output.squeeze(0)\n","        prediction = self.fc(output)\n","        #prediction = [batch size, output dim]\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","        return prediction, hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gCorOYa333cm"},"source":["### Seq2Seq\n","\n","Para la parte final de la implementación, desarrollaremos el modelo seq2seq. Este modelo se encargará de:\n","- recibir la frase de entrada/origen\n","- usar el encoder para producir los vectores de contexto\n","- usar el decoder para producir la predicción de la frase de salida/destino\n","\n","Nuestro modelo completo se verá así:\n","\n","![](https://i.imgur.com/uTI0sgy.png)\n","\n","El modelo `Seq2Seq` incluye un `Encoder`, un `Decoder` y un `device` (utilizado para colocar tensores en la GPU, si hubiera).\n","\n","Para esta implementación, tenemos que asegurarnos de que el número de capas y las dimensiones ocultas (y de celda) sean iguales en el `Encoder` y en el `Decoder`. Este no es siempre el caso, no necesariamente tenemos que tener el mismo número de capas o los mismos tamaños de dimensión ocultos en un modelo secuencia a secuencia. Sin embargo, si hiciéramos algo como tener un número diferente de capas, entonces tendríamos que tomar decisiones sobre cómo se maneja esto. Por ejemplo, si nuestro encoder tiene 2 capas y nuestro decoder solo tiene 1, ¿cómo se maneja esto? ¿Promediamos los dos vectores de contexto generados por el encoder? ¿Pasamos ambos por una capa lineal? ¿Usamos solo el vector de contexto de la capa más alta? Etc.\n","\n","Nuestro método `forward` toma la oración de origen, la oración de destino y un ratio de forzamiento del maestro. Esta última se utiliza al entrenar nuestro modelo. Al decodificar, en cada paso de tiempo predeciremos cuál será el siguiente token en la secuencia objetivo a partir de los tokens decodificados previamente, $ \\hat{y}_{t + 1} = f(s_t ^ L) $. Con una probabilidad igual al ratio de forzamiento del maestro (`teacher_forcing_ratio`) usaremos el token de ground truth como entrada al decodificador durante el siguiente paso de tiempo. Sin embargo, con probabilidad `1 - teacher_forcing_ratio`, usaremos el token que el modelo predijo como la próxima entrada al modelo, incluso si no coincide con el siguiente token ground truth.\n","\n","Lo primero que hacemos en el método `forward` es crear un tensor de `output` que almacenará todas nuestras predicciones, $ \\hat{Y} $.\n","\n","Luego alimentamos la oración de entrada/origen, `src`, en el encoder y recibimos los estados finales ocultos y de celda.\n","\n","La primera entrada al decoder es el token de inicio de secuencia (`<sos>`). Como nuestro tensor `trg` ya tiene el token` <sos> `adjunto (desde que definimos el `init_token` en nuestro Field `TRG`) obtenemos nuestro $ y_1 $ haciendo un slice. Sabemos qué tan largo deben ser nuestras oraciones de destino (`max_len`), por lo que lo repetimos esa cantidad de veces. El último token de entrada en el decoder es el token **anterior** `<eos>` - el token `<eos>` **nunca** se ingresa en el decoder.\n","\n","Durante cada iteración del ciclo, nosotros:\n","- pasamos la entrada, los estados ocultos y de la celda anteriores ($ y_t, s_{t-1}, c_{t-1} $) al decoder\n","- recibir una predicción, el siguiente estado oculto y el siguiente estado de celda ($ \\hat{y}_{t + 1}, s_{t}, c_{t} $) del decoder\n","- colocar nuestra predicción, $ \\hat{y}_{t + 1} $ / `output` en nuestro tensor de predicciones, $ \\hat{Y} $ / `outputs`\n","- decidir si vamos a hacer un forzamiento del maestro o no\n","     - si lo hacemos, el siguiente `input` es el siguiente token de ground truth en la secuencia, $ y_{t + 1} $  `trg[t]`\n","     - si no lo hacemos, el siguiente `input` es el siguiente token predicho en la secuencia, $ \\hat{y}_{t + 1} $ `top1`, que obtenemos haciendo un `argmax` sobre el tensor de salida\n","    \n","Una vez que hemos hecho todas nuestras predicciones, devolvemos nuestro tensor lleno de predicciones, $ \\hat{Y} $ / `outputs`.\n","\n","**Nota**: nuestro bucle de decoder comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de `outputs` permanece solo en ceros. Así que nuestros `trg` y` outputs` se parecen a:\n","\n","$$\\begin{align*}\n","\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n","\\text{outputs} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n","\\end{align*}$$\n","\n","Más adelante, cuando calculamos la pérdida, cortamos el primer elemento de cada tensor para obtener:\n","\n","$$\\begin{align*}\n","\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n","\\text{outputs} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n","\\end{align*}$$"]},{"cell_type":"code","metadata":{"id":"DGC3wq5h33cn"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el modelo seq2seq con la  #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","        assert encoder.hid_dim == decoder.hid_dim, \\\n","            \"Las dimensiones de las variables ocultas del encoder y el decoder deben ser iguales!\"\n","        assert encoder.n_layers == decoder.n_layers, \\\n","            \"El encoder y el decoder deben tener el mismo número de capas!\"\n","        \n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio es la probabilidad con la que se usa el forzamiento del maestro\n","        #e.g. si teacher_forcing_ratio es 0.75 usamos las etiquetas el 75% del tiempo\n","        \n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor para almacenar las salidas del decoder \n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #el último estado oculto del encoder se usa como estado oculto inicial del decoder\n","        hidden, cell = self.encoder(src)\n","        \n","        #la primera entrada al decoder es el token <sos>\n","        input = trg[0,:]\n","        \n","        for t in range(1, trg_len):\n","            ########################################################################\n","            # TODO: Implementá el bucle de decodificación dentro de la función     #\n","            # forward para el modelo seq2seq. Deberías usar las capas que definiste#     \n","            # en __init__ y especificar la conectividad de dichas capas            #\n","            ########################################################################\n","            # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","                \n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[t] if teacher_force else top1\n","\n","            # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","            ########################################################################\n","            #                          FINAL DE TU CÓDIGO                          #       \n","            ########################################################################\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jHa_YQQg33co"},"source":["## Entrenamiento del modelo Seq2Seq\n","\n","Ahora que tenemos nuestro modelo implementado, podemos comenzar a entrenarlo.\n","\n","Primero, inicializaremos nuestro modelo. Como se mencionó anteriormente, las dimensiones de entrada y salida están definidas por el tamaño del vocabulario. Las dimensiones de embedding y el dropout pueden ser diferentes para el encoder y el decoder, pero el número de capas y el tamaño de los estados ocultos/de celda deben ser los mismos.\n","\n","Luego definimos el encoder, decoder y luego nuestro modelo Seq2Seq, que colocamos en el `device`."]},{"cell_type":"code","metadata":{"id":"uaH2x_et33co"},"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","HID_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIIuLTMn33co"},"source":["El siguiente paso es inicializar los pesos de nuestro modelo. En el paper, afirman que inicializan todos los pesos a partir de una distribución uniforme entre -0,08 y +0,08, es decir, $ \\mathcal{U}(- 0,08, 0,08) $.\n","\n","Inicializamos pesos en PyTorch creando una función que \"aplicamos\" a nuestro modelo. Cuando usemos `apply`, la función` init_weights` será llamada en cada módulo y submódulo dentro de nuestro modelo. Para cada módulo, recorremos todos los parámetros y los muestreamos a partir de una distribución uniforme con `nn.init.uniform_`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhvl8yGK33co","executionInfo":{"status":"ok","timestamp":1623154452617,"user_tz":180,"elapsed":385,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"f081c740-5e83-41c3-eb7c-edb0858a71d1"},"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","        \n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (embedding): Embedding(7855, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","  )\n","  (decoder): Decoder(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (embedding): Embedding(5893, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (fc): Linear(in_features=512, out_features=5893, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"TxK4zc4533cp"},"source":["También definimos una función que calculará el número de parámetros entrenables en el modelo."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNGYccZ933cp","executionInfo":{"status":"ok","timestamp":1623154468027,"user_tz":180,"elapsed":360,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"5e06ce57-7519-46dc-da7a-21dff63cd880"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 13,899,013 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o0-X5vdh33cq"},"source":["Definimos nuestro optimizador, que usamos para actualizar nuestros parámetros en el ciclo de entrenamiento. Consulte [esta](http://ruder.io/optimizing-gradient-descent/) publicación para obtener información sobre los diferentes optimizadores. Aquí, usaremos Adam."]},{"cell_type":"code","metadata":{"id":"H7ixOiCO33cq"},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"we_mAxk333cq"},"source":["A continuación, definimos nuestra función de pérdida. La función `CrossEntropyLoss` calcula tanto el log softmax como la probabilidad logarítmica negativa de nuestras predicciones.\n","\n","Nuestra función de pérdida calcula la pérdida promedio por token, sin embargo, al pasar el índice del token `<pad>` como el argumento `ignore_index`, ignoramos la pérdida siempre que el token de destino sea un token de relleno."]},{"cell_type":"code","metadata":{"id":"SEfWpAhK33cq"},"source":["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ou6Xvxgq33cr"},"source":["A continuación, definiremos nuestro ciclo de entrenamiento.\n","\n","Primero, configuraremos el modelo en \"modo de entrenamiento\" con `model.train()`. Esto activará el dropout (y la normalización por lotes, que no estamos usando) y luego iterará a través de nuestro iterador de datos.\n","\n","Como se dijo antes, nuestro bucle de decodificador comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de `outputs` permanece en ceros. Así que nuestros `trg` y `outputs` se parecen a:\n","\n","$$\\begin{align*}\n","\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n","\\text{outputs} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n","\\end{align*}$$\n","\n","Aquí, cuando calculamos la pérdida, cortamos el primer elemento de cada tensor para obtener:\n","\n","$$\\begin{align*}\n","\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n","\\text{outputs} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n","\\end{align*}$$\n","\n","En cada iteración:\n","- obtenemos las oraciones de origen y destino del lote, $ X $ y $ Y $\n","- ponemos a cero los gradientes calculados a partir del último lote\n","- introducimos el origen y el destino en el modelo para obtener la predicción, $ \\hat{Y} $\n","- como la función de pérdida solo funciona en entradas 2d con objetivos 1d, necesitamos aplanar cada una de ellas con `.view`\n","     - cortamos la primera columna de la salida y las oraciones destino como se mencionó anteriormente\n","- calculamos los gradientes con `loss.backward()`\n","- recortamos los gradientes para evitar que exploten (un problema común en las RNN)\n","- actualizamos los parámetros de nuestro modelo haciendo un paso de optimización\n","- sumamos el valor de la pérdida a un total acumulado\n","\n","Finalmente, devolvemos la pérdida que se promedia en todos los lotes."]},{"cell_type":"code","metadata":{"id":"s2-ty12c33cr"},"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","        \n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","        \n","        output_dim = output.shape[-1]\n","        \n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","        \n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xIiXkm833cs"},"source":["Nuestro ciclo de evaluación es similar a nuestro ciclo de entrenamiento, sin embargo, como no estamos actualizando ningún parámetro, no necesitamos pasar un optimizador o un valor de clip.\n","\n","Debemos recordar configurar el modelo en modo de evaluación con `model.eval()`. Esto desactivará el dropout (y la normalización de lotes, si se usa).\n","\n","Usamos el bloque `with torch.no_grad()` para asegurarnos de que no se calculen gradientes dentro del bloque. Esto reduce el consumo de memoria y acelera las cosas.\n","\n","El loop es similar (sin las actualizaciones de los parámetros), sin embargo, debemos asegurarnos de desactivar el forzamiento del maestro para la evaluación. Esto hará que el modelo solo use sus propias predicciones para hacer más predicciones dentro de una oración, lo que refleja cómo se usaría en  producción."]},{"cell_type":"code","metadata":{"id":"2mH28upa33cs"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #desactiva el forzamiento del maestro\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","            \n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHq7uCu433cs"},"source":["A continuación, crearemos una función que usaremos para decirnos cuánto tarda una época."]},{"cell_type":"code","metadata":{"id":"q2EBOZ-933cs"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJtg0KrT33ct"},"source":["¡Por fin podemos empezar a entrenar nuestro modelo!\n","\n","En cada época, comprobaremos si nuestro modelo ha logrado la mejor pérdida de validación hasta el momento. Si es así, actualizaremos nuestra mejor pérdida de validación y guardaremos los parámetros de nuestro modelo (llamado `state_dict` en PyTorch). Luego, cuando vayamos a probar nuestro modelo (con el dataset de test), usaremos los parámetros guardados para lograr la mejor pérdida de validación.\n","\n","Imprimiremos tanto la pérdida como la perplejidad en cada época. Es más fácil ver un cambio en la perplejidad que un cambio en la pérdida, ya que los números son mucho mayores."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4GWiiMbf33ct","executionInfo":{"status":"ok","timestamp":1623154871582,"user_tz":180,"elapsed":371818,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"2286fe97-9738-4722-e71c-0e36c85158c7"},"source":["N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 36s\n","\tTrain Loss: 5.043 | Train PPL: 155.008\n","\t Val. Loss: 5.279 |  Val. PPL: 196.091\n","Epoch: 02 | Time: 0m 36s\n","\tTrain Loss: 4.495 | Train PPL:  89.591\n","\t Val. Loss: 4.773 |  Val. PPL: 118.220\n","Epoch: 03 | Time: 0m 37s\n","\tTrain Loss: 4.164 | Train PPL:  64.311\n","\t Val. Loss: 4.539 |  Val. PPL:  93.594\n","Epoch: 04 | Time: 0m 37s\n","\tTrain Loss: 3.946 | Train PPL:  51.733\n","\t Val. Loss: 4.395 |  Val. PPL:  81.013\n","Epoch: 05 | Time: 0m 37s\n","\tTrain Loss: 3.770 | Train PPL:  43.370\n","\t Val. Loss: 4.295 |  Val. PPL:  73.316\n","Epoch: 06 | Time: 0m 37s\n","\tTrain Loss: 3.621 | Train PPL:  37.361\n","\t Val. Loss: 4.240 |  Val. PPL:  69.422\n","Epoch: 07 | Time: 0m 37s\n","\tTrain Loss: 3.475 | Train PPL:  32.283\n","\t Val. Loss: 4.157 |  Val. PPL:  63.895\n","Epoch: 08 | Time: 0m 37s\n","\tTrain Loss: 3.354 | Train PPL:  28.629\n","\t Val. Loss: 4.053 |  Val. PPL:  57.559\n","Epoch: 09 | Time: 0m 36s\n","\tTrain Loss: 3.216 | Train PPL:  24.931\n","\t Val. Loss: 3.961 |  Val. PPL:  52.503\n","Epoch: 10 | Time: 0m 37s\n","\tTrain Loss: 3.083 | Train PPL:  21.815\n","\t Val. Loss: 3.977 |  Val. PPL:  53.374\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TKAhWvpS33ct"},"source":["Cargaremos los parámetros (`state_dict`) que le dieron a nuestro modelo la mejor pérdida de validación y lo ejecutaremos en el conjunto de prueba."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XcmYXqX33cu","executionInfo":{"status":"ok","timestamp":1623154889665,"user_tz":180,"elapsed":867,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"d71cb8f0-3b85-4a48-ac9b-0eed6fedc918"},"source":["model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 3.937 | Test PPL:  51.276 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W0e2UhVa33cu"},"source":["En la siguiente sección implementaremos un modelo que logra una perplejidad de prueba mejorada, pero solo usa una sola capa en el encoder y el decoder."]},{"cell_type":"markdown","metadata":{"id":"JZaVi_bhTnbf"},"source":["# Sección 2 - Reduciendo la Compresión de Información\n","En esta segunda sección sobre modelos secuencia a secuencia usando PyTorch y TorchText, implementaremos el modelo de [Aprendizaje de representaciones de frases usando RNN Encoder-Decoder para traducción automática estadística](https://arxiv.org/abs/1406.1078). Este modelo logrará una perplejidad de prueba mejorada utilizando solo un RNN de una sola capa tanto en el codificador como en el decodificador."]},{"cell_type":"markdown","metadata":{"id":"hVjg1R7XFh_o"},"source":["## Introducción\n","\n","Recordemos el modelo general de encoder-decoder.\n","\n","![](https://i.imgur.com/09LRqMD.png)\n","\n","Usamos nuestro encoder (verde) sobre los embedding de la secuencia de origen (amarillo) para crear un vector de contexto (rojo). Luego usamos ese vector de contexto con el decoder (azul) y una capa lineal (violeta) para generar la oración de destino.\n","\n","En el modelo anterior, usamos una LSTM multicapa como encoder y decoder.\n","\n","![](https://i.imgur.com/uTI0sgy.png)\n","\n","Una desventaja del modelo de la sección anterior es que el decoder está tratando de meter mucha información en los estados ocultos. Durante la decodificación, el estado oculto deberá contener información sobre la totalidad de la secuencia fuente, así como todos los tokens que se han decodificado hasta ahora. Al aliviar parte de esta compresión de información, ¡podemos crear un modelo mejor!\n","\n","También usaremos una GRU (Gated Recurrent Unit) en lugar de una LSTM (Long Short-Term Memory). ¿Por qué? Principalmente porque eso es lo que hicieron en el papaer (este paper también introdujo GRU) y también porque usamos LSTM en la sección anterior. Para comprender en qué se diferencian los GRU (y los LSTM) del RNNS estándar, consulte [este enlace](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). ¿Es un GRU mejor que un LSTM? [La investigación](https://arxiv.org/abs/1412.3555) ha demostrado que son prácticamente iguales, y ambos son mejores que los RNN estándar. \n"]},{"cell_type":"code","metadata":{"id":"09GyJd2-U-79"},"source":["BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort=False, \n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpDNAeZZVUpx"},"source":["## Construyendo el modelo Seq2Seq"]},{"cell_type":"markdown","metadata":{"id":"_JGTfV1NFiA7"},"source":["\n","### Encoder\n","\n","El codificador es similar al anterior, con el LSTM multicapa intercambiado por un GRU de una sola capa. Tampoco pasamos el dropout como un argumento al GRU ya que ese dropout se usa entre cada capa de un RNN multicapa. Como solo tenemos una capa, PyTorch mostrará una advertencia si intentamos pasarle un valor de dropout.\n","\n","Otra cosa a tener en cuenta sobre las GRU es que solo requieren y devuelven un estado oculto, no hay un estado de celda como en el LSTM.\n","\n","$$\\begin{align*}\n","h_t &= \\text{GRU}(e(x_t), h_{t-1})\\\\\n","(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\\\\\n","h_t &= \\text{RNN}(e(x_t), h_{t-1})\n","\\end{align*}$$\n","\n","De las ecuaciones anteriores, parece que el RNN y el GRU son idénticos. Dentro del GRU, sin embargo, hay una serie de *mecanismos de compuertas* que controlan el flujo de información hacia y desde el estado oculto (similar a un LSTM). Nuevamente, para obtener más información, consulte [esta excelente publicación]https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n","\n","El resto del encoder debería ser muy familiar desde la sección anterior, toma como entrada una secuencia, $ X = \\{x_1, x_2, ..., x_T \\} $, la pasa a través de la capa de embedding, calcula de forma recurrente estados ocultos, $ H = \\{h_1, h_2, ..., h_T \\} $, y devuelve un vector de contexto (el estado oculto final), $ z = h_T $.\n","\n","$$h_t = \\text{EncoderGRU}(e(x_t), h_{t-1})$$\n","\n","Esto es idéntico al encoder del modelo general seq2seq, con toda la \"magia\"  ocurriendo dentro del GRU (verde).\n","\n","![](https://i.imgur.com/Ben5gcM.png)"]},{"cell_type":"code","metadata":{"id":"1FVz6b2zFiA8"},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el encoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.dropout = nn.Dropout(dropout)\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.GRU(emb_dim, hid_dim)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, src):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el encoder. Deberías        #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        \n","\n","        #src = [src len, batch size]\n","        \n","        embedded = self.embedding(src)\n","        embedded = self.dropout(embedded)\n","        \n","        #embedded = [src len, batch size, emb dim]\n","        \n","        outputs, hidden = self.rnn(embedded)\n","        \n","        #outputs = [src len, batch size, hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        \n","        #los outputs provienen siempre de la capa oculta superior\n","        \n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","\n","        return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyO_WKQgFiA9"},"source":["### Decoder\n","\n","El decoder es donde la implementación difiere significativamente del modelo anterior y aliviamos algo de la compresión de información.\n","\n","En lugar de que el GRU en el decoder tome solo el embedding del token de destino, $ d(y_t) $ y el estado oculto anterior $ s_ t-1} $ como entradas, también toma el vector de contexto $ z $.\n","\n","$$s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z)$$\n","\n","Observe cómo este vector de contexto, $ z $, no tiene un subíndice $ t $, lo que significa que reutilizamos el mismo vector de contexto devuelto por el encoder para cada paso de tiempo en el decoder.\n","\n","Antes, predijimos el siguiente token, $ \\hat{y}_{t + 1} $, con la capa lineal, $ f $, solo usando el estado oculto del decoder de la capa superior en ese paso de tiempo, $ s_t $, como $ \\hat{y}_{t + 1} = f(s_t ^ L) $. Ahora, también pasamos el embedding del token actual, $ d(y_t) $ y el vector de contexto, $ z $ a la capa lineal.\n","\n","$$\\hat{y}_{t+1} = f(d(y_t), s_t, z)$$\n","\n","Por lo tanto, nuestro decodificador ahora se parece a esto:\n","\n","![](https://i.imgur.com/V72jlPu.png)\n","\n","Tenga en cuenta que el estado oculto inicial, $ s_0 $, sigue siendo el vector de contexto, $ z $, por lo que al generar el primer token, en realidad estamos ingresando dos vectores de contexto idénticos en el GRU.\n","\n","¿Cómo estos dos cambios reducen la compresión de la información? Bueno, hipotéticamente, los estados ocultos del decoder, $ s_t $, ya no necesitan contener información sobre la secuencia de origen, ya que siempre está disponible como entrada. Por lo tanto, solo necesita contener información sobre los tokens que ha generado hasta ahora. La adición de $ y_t $ a la capa lineal también significa que esta capa puede ver directamente cuál es el token, sin tener que obtener esta información del estado oculto. \n","\n","Sin embargo, esta hipótesis es solo una hipótesis, es imposible determinar cómo el modelo realmente usa la información que se le proporciona (no escuche a nadie que diga lo contrario). Sin embargo, es una intuición sólida y los resultados parecen indicar que estas modificaciones son una buena idea.\n","\n","Dentro de la implementación, pasaremos $ d (y_t) $ y $ z $ al GRU al concatenarlos juntos, por lo que las dimensiones de entrada al GRU ahora son `emb_dim + hid_dim` (ya que el vector de contexto será de tamaño ` hid_dim` ). La capa lineal tomará $ d (y_t), s_t $ y $ z $ también al concatenarlos juntos, por lo tanto, las dimensiones de entrada ahora son `emb_dim + hid_dim * 2`. Tampoco pasamos un valor de dropout al GRU, ya que solo usa una capa.\n","\n","`forward` ahora toma un argumento `context`. Dentro de `forward`, concatenamos $ y_t $ y $ z $ como` emb_con` antes de alimentar al GRU, y concatenamos $ d (y_t) $, $ s_t $ y $ z $ juntos como `output` antes de alimentarlo a través de la capa lineal para recibir nuestras predicciones, $ \\hat{y} _ {t + 1} $."]},{"cell_type":"code","metadata":{"id":"QGVW5U26FiA-"},"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el decoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.output_dim = output_dim\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n","        self.fc = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","\n","        \n","    def forward(self, input, hidden, context):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el decoder. Deberías        #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        #input = [batch size]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        #context = [n layers * n directions, batch size, hid dim]\n","        \n","        #n layers y n directions en el decoder valdrán ambos siempre 1, por lo tanto:\n","        #hidden = [1, batch size, hid dim]\n","        #context = [1, batch size, hid dim]\n","        \n","        input = input.unsqueeze(0)\n","        \n","        #input = [1, batch size]\n","        \n","        embedded = self.embedding(input)\n","        embedded = self.dropout(embedded)\n","        \n","        #embedded = [1, batch size, emb dim]\n","                \n","        emb_con = torch.cat((embedded, context), dim = 2)\n","            \n","        #emb_con = [1, batch size, emb dim + hid dim]\n","            \n","        output, hidden = self.rnn(emb_con, hidden)\n","        \n","        #output = [seq len, batch size, hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        \n","        #seq len, n layers and n directions valdrán siempre 1, por lo tanto:\n","        #output = [1, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","        \n","        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0),\n","                            context.squeeze(0)), dim = 1)\n","        \n","        #output = [batch size, emb dim + hid dim * 2]\n","        \n","        prediction = self.fc(output)\n","        \n","        #prediction = [batch size, output dim]\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        return prediction, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IMvQ1mQDFiA_"},"source":["### Modelo Seq2Seq\n","\n","Poniendo el encoder y el decoder juntos, obtenemos:\n","\n","![](https://i.imgur.com/uzewKDu.png)\n","\n","Nuevamente, en esta implementación debemos asegurarnos de que las dimensiones ocultas tanto en el encoder como en el decoder sean las mismas.\n","\n","Repasando brevemente todos los pasos:\n","- el tensor de `outputs` se crea para contener todas las predicciones, $ \\hat {Y} $\n","- la secuencia de origen, $ X $, se alimenta al encoder para recibir un vector de contexto `context`\n","- el estado oculto del decoder inicial se establece en el vector `context`, $ s_0 = z = h_T $\n","- usamos un lote de tokens `<sos>` como la primera `entrada`, $ y_1 $\n","- luego decodificamos dentro de un bucle:\n","   - insertando el token de entrada $ y_t $, estado oculto anterior, $ s_ {t-1} $, y el vector de contexto, $ z $, en el decoder\n","   - recibiendo una predicción, $\\hat{y}_{t + 1}$, y un nuevo estado oculto, $ s_t$\n","   - luego decidimos si vamos a hacer forzamiento del maestro o no, estableciendo la siguiente entrada según corresponda (ya sea el siguiente token de ground truth en la secuencia objetivo o el próximo token predicho más alto)"]},{"cell_type":"code","metadata":{"id":"EOnzb12WFiA_"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el modelo seq2seq con la  #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","        assert encoder.hid_dim == decoder.hid_dim, \\\n","            \"Las dimensiones de las variables ocultas del encoder y el decoder deben ser iguales!\"\n","        \n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio es la probabilidad con la que se usa el forzamiento del maestro\n","        #e.g. si teacher_forcing_ratio es 0.75 usamos las etiquetas el 75% del tiempo\n","        \n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor para almacenar las salidas del decoder \n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #el último estado oculto del encoder se usa como estado oculto inicial del decoder\n","        context = self.encoder(src)\n","        \n","        #el contexto también es usado como el estado oculto inicial del decoder\n","        hidden = context\n","        \n","        #la primera entrada al decoder es el token <sos>\n","        input = trg[0,:]\n","        \n","        for t in range(1, trg_len):\n","            ########################################################################\n","            # TODO: Implementá el bucle de decodificación dentro de la función     #\n","            # forward para el modelo seq2seq. Deberías usar las capas que definiste#     \n","            # en __init__ y especificar la conectividad de dichas capas            #\n","            ########################################################################\n","            # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","                \n","            output, hidden = self.decoder(input, hidden, context)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1) \n","            input = trg[t] if teacher_force else top1\n","\n","            # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","            ########################################################################\n","            #                          FINAL DE TU CÓDIGO                          #       \n","            ########################################################################\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7sN6EN2NFiBA"},"source":["## Entrenamiento del modelo Seq2Seq\n","\n","El resto de esta sección es muy similar a la anterior.\n","\n","Inicializamos nuestro encoder, decoder y modelo seq2seq (colocándolo en la GPU si tenemos una). Como antes, las dimensiones de embedding y la cantidad de dropout utilizada pueden ser diferentes entre el encoder y el decoder, pero las dimensiones ocultas deben seguir siendo las mismas."]},{"cell_type":"code","metadata":{"id":"lpTB3v90FiBA"},"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","HID_DIM = 512\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uW2h8xaFiBA"},"source":["A continuación, inicializamos nuestros parámetros. El documento establece que los parámetros se inicializan a partir de una distribución normal con una media de 0 y una desviación estándar de 0.01, es decir, $ \\mathcal{N}(0, 0.01) $.\n","\n","También establece que debemos inicializar los parámetros recurrentes a una inicialización especial, sin embargo, para mantener las cosas simples, también los inicializaremos en $ \\mathcal{N}(0, 0.01) $."]},{"cell_type":"code","metadata":{"id":"FglAowkZFiBA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623158992005,"user_tz":180,"elapsed":314,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"a002213a-d797-46a1-9381-601b1c3a8b13"},"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.normal_(param.data, mean=0, std=0.01)\n","        \n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (embedding): Embedding(7855, 256)\n","    (rnn): GRU(256, 512)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5893, 256)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (rnn): GRU(768, 512)\n","    (fc): Linear(in_features=1280, out_features=5893, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"GnGF4F9UFiBC"},"source":["Imprimimos el número de parámetros.\n","\n","Aunque solo tenemos un RNN de una sola capa para nuestro encoder y decoder, en realidad tenemos **más** parámetros que el último modelo. Esto se debe al mayor tamaño de las entradas al GRU y la capa lineal. Sin embargo, no es una cantidad significativa de parámetros y provoca un aumento mínimo en el tiempo de entrenamiento (~ 3 segundos extras por época)."]},{"cell_type":"code","metadata":{"id":"uacGKjxeFiBC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623158995266,"user_tz":180,"elapsed":310,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"c5f4ff21-6d15-4a4f-d97b-893b98e7761a"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 14,220,293 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kDx4z4e_FiBC"},"source":["Inicializamos el optimizador"]},{"cell_type":"code","metadata":{"id":"kFl9v2cQFiBD"},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXRwJ7-aFiBD"},"source":["También inicializamos la función de pérdida, asegurándonos de ignorar la pérdida en los tokens `<pad>`."]},{"cell_type":"code","metadata":{"id":"HGehDGsMFiBE"},"source":["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9IS1bvnFiBG"},"source":["Luego creamos el ciclo de entrenamiento ..."]},{"cell_type":"code","metadata":{"id":"znhL4bNFFiBH"},"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","        \n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","        \n","        output_dim = output.shape[-1]\n","        \n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","        \n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0NGCGv5FiBK"},"source":["... y el ciclo de evaluación, recordando configurar el modelo en modo `eval` y desactivar el forzamiento del maestro."]},{"cell_type":"code","metadata":{"id":"-oy-guijFiBL"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #desactiva el forzamiento del maestro\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btmRjxd0FiBL"},"source":["También definiremos la función que calcula cuánto tarda una época."]},{"cell_type":"code","metadata":{"id":"EkS1ZNG6FiBM"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFZfSyCYFiBM"},"source":["Luego, entrenamos nuestro modelo, guardando los parámetros que nos den la mejor pérdida de validación."]},{"cell_type":"code","metadata":{"id":"5XXbNGx5FiBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623159377181,"user_tz":180,"elapsed":364252,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"e2a756dd-7eb3-425b-fa6a-ec678b4c3a9f"},"source":["N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut2-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 36s\n","\tTrain Loss: 5.006 | Train PPL: 149.376\n","\t Val. Loss: 4.989 |  Val. PPL: 146.817\n","Epoch: 02 | Time: 0m 36s\n","\tTrain Loss: 4.311 | Train PPL:  74.543\n","\t Val. Loss: 4.721 |  Val. PPL: 112.280\n","Epoch: 03 | Time: 0m 36s\n","\tTrain Loss: 4.004 | Train PPL:  54.841\n","\t Val. Loss: 4.531 |  Val. PPL:  92.845\n","Epoch: 04 | Time: 0m 36s\n","\tTrain Loss: 3.710 | Train PPL:  40.845\n","\t Val. Loss: 4.280 |  Val. PPL:  72.249\n","Epoch: 05 | Time: 0m 36s\n","\tTrain Loss: 3.400 | Train PPL:  29.959\n","\t Val. Loss: 4.117 |  Val. PPL:  61.375\n","Epoch: 06 | Time: 0m 36s\n","\tTrain Loss: 3.125 | Train PPL:  22.754\n","\t Val. Loss: 4.014 |  Val. PPL:  55.386\n","Epoch: 07 | Time: 0m 36s\n","\tTrain Loss: 2.892 | Train PPL:  18.024\n","\t Val. Loss: 3.861 |  Val. PPL:  47.492\n","Epoch: 08 | Time: 0m 35s\n","\tTrain Loss: 2.715 | Train PPL:  15.109\n","\t Val. Loss: 3.781 |  Val. PPL:  43.870\n","Epoch: 09 | Time: 0m 36s\n","\tTrain Loss: 2.483 | Train PPL:  11.976\n","\t Val. Loss: 3.820 |  Val. PPL:  45.604\n","Epoch: 10 | Time: 0m 36s\n","\tTrain Loss: 2.311 | Train PPL:  10.087\n","\t Val. Loss: 3.742 |  Val. PPL:  42.193\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_ShZVhwBFiBN"},"source":["Finalmente, probamos el modelo en el conjunto de prueba usando estos \"mejores\" parámetros."]},{"cell_type":"code","metadata":{"id":"4fecZ69_FiBO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623159385579,"user_tz":180,"elapsed":754,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"a55043cc-1c0f-47b9-8d33-38cbf2441e94"},"source":["model.load_state_dict(torch.load('tut2-model.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 3.689 | Test PPL:  40.021 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wp3tF1PEFiBO"},"source":["Con solo mirar la pérdida de prueba, obtenemos un mejor rendimiento que el modelo anterior. ¡Esta es una muy buena señal de que esta arquitectura modelo está haciendo algo bien! Aliviar la compresión de la información parece la forma de hacerlo, y en el próximo tutorial ampliaremos esto aún más con *atención*."]},{"cell_type":"markdown","metadata":{"id":"0UVy-sl2em9x"},"source":["# Sección 3 - Metiendo Atención\n","\n","En esta tercer sección sobre modelos de secuencia a secuencia usando PyTorch y TorchText, implementaremos el modelo de [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). Este modelo alcanza nuestra mayor perplejidad hasta el momento, ~ 27 en comparación con ~ 34 para el modelo anterior.\n"]},{"cell_type":"markdown","metadata":{"id":"FugBRGI4E2Ob"},"source":["## Introducción\n","\n","Como recordatorio, aquí está el modelo general de encoder-decoder:\n","\n","![](https://i.imgur.com/09LRqMD.png)\n","\n","En la sección anterior, nuestra arquitectura se configuró para reducir la \"compresión de información\" pasando explícitamente el vector de contexto, $ z $, al decoder en cada paso de tiempo y pasando tanto el vector de contexto como el embedding de la palabra de entrada. , $ d (y_t) $, junto con el estado oculto, $ s_t $, a la capa lineal, $ f $, para hacer una predicción.\n","\n","![](https://i.imgur.com/3k7EnTX.png)\n","\n","Aunque hemos reducido parte de esta compresión, nuestro vector de contexto aún necesita contener toda la información sobre la oración fuente. El modelo implementado en esta sección evita esta compresión al permitir que el decodificador vea la oración origen completa (a través de sus estados ocultos) en cada paso de decodificación. ¿Como hace esto? Utiliza *atención*.\n","\n","La atención funciona primero, calculando un vector de atención, $a$, que tiene la misma longitud de la oración origen. El vector de atención tiene la propiedad de que cada elemento está entre 0 y 1, y el vector completo suma 1. Luego calculamos una suma ponderada de los estados ocultos de nuestra oración origen, $ H $, para obtener un vector fuente ponderado, $ w $ .\n","\n","$$w = \\sum_{i}a_ih_i$$\n","\n","Calculamos un nuevo vector origen ponderado en cada paso de tiempo al decodificar, usándolo como entrada para nuestro decoder RNN, así como la capa lineal para hacer una predicción. Explicaremos cómo hacer todo esto durante la sección.\n"]},{"cell_type":"markdown","metadata":{"id":"0PVNsWtxV47I"},"source":["## Construyendo el modelo Seq2Seq"]},{"cell_type":"markdown","metadata":{"id":"6KILAZXFE2Oz"},"source":["### Encoder\n","\n","Primero, construiremos el encoder. Al igual que en el modelo anterior, solo usamos un GRU de una sola capa, sin embargo ahora usamos un *RNN bidireccional*. Con un RNN bidireccional, tenemos dos RNN en cada capa. Un *RNN hacia adelante* que repasa los embedding de la oración de izquierda a derecha (que se muestra a continuación en verde), y un *RNN hacia atrás* que repasa los embedding de la oración de derecha a izquierda (verde azulado). Todo lo que necesitamos hacer en el código es establecer `bidirectional = True` y luego pasar los embedding de la oración al RNN como antes.\n","\n","![](https://i.imgur.com/bdUnvj9.png)\n","\n","Ahora tenemos:\n","\n","$$\\begin{align*}\n","h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n","h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n","\\end{align*}$$\n","\n","Donde $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n","\n","Como antes, solo pasamos una entrada(`embedded`) al RNN, que le dice a PyTorch que inicialice los estados ocultos iniciales hacia adelante y hacia atrás ($ h_0 ^ \\rightarrow $ y $ h_0 ^ \\leftarrow $, respectivamente) como un tensor de todos los ceros. También obtendremos dos vectores de contexto, uno del RNN hacia adelante después de haber visto la última palabra en la oración, $ z ^ \\rightarrow = h_T ^ \\rightarrow $, y uno del RNN hacia atrás después de haber visto la primera palabra en la oración, $ z ^ \\leftarrow = h_T ^ \\leftarrow $.\n","\n","El RNN devuelve `outputs` y `hidden`.\n","\n","`outputs` es de tamaño **[longitud de la oración de origen, tamaño de lote, (dimensión de variables ocultas)*(numero de direcciones)] ** donde los primeros ` hid_dim` elementos en el tercer eje son los estados ocultos de la capa superior hacia adelante RNN, y los últimos `hid_dim ` elementos son estados ocultos de la capa superior hacia atrás RNN. Podemos pensar en el tercer eje como los estados ocultos hacia adelante y hacia atrás concatenados entre sí, es decir, $ h_1 = [h_1 ^ \\rightarrow; h_{T} ^ \\leftarrow] $, $ h_2 = [h_2 ^ \\rightarrow; h_{T-1} ^ \\leftarrow] $ y podemos denotar todos los estados ocultos del encoder (hacia adelante y hacia atrás concatenados juntos) como $ H = \\{h_1, h_2, ..., h_T \\} $. \n","\n","`hidden` es de tamaño **[número de capas * número de direcciones, tamaño de lote, dimensión de variables ocultas]**, donde **[- 2,:,:]** entrega el estado oculto de la capa superior de la RNN hacia adelante después del paso de tiempo final (es decir, después de haber visto la última palabra en la oración) y ** [- 1,:,: ]** entrega el estado oculto de la capa superior de la RNN hacia atrás después del paso de tiempo final (es decir, después de haber visto la primera palabra en la oración).\n","\n","Como el decoder no es bidireccional, solo necesita un único vector de contexto, $ z $, para usar como su estado oculto inicial, $ s_0 $, y actualmente tenemos dos, uno hacia adelante y uno hacia atrás ($ z ^ \\rightarrow = h_T ^ \\rightarrow $ y $ z ^ \\leftarrow = h_T ^ \\leftarrow $, respectivamente). Resolvemos esto concatenando los dos vectores de contexto juntos, pasándolos a través de una capa lineal, $ g $, y aplicando la función de activación $ \\tanh $. \n","\n","$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n","\n","Como queremos que nuestro modelo mire hacia atrás sobre la totalidad de la oración origen, devolvemos `outputs`, los estados ocultos hacia adelante y hacia atrás para cada token en la oración origen apilados. También devolvemos `hidden`, que actúa como nuestro estado oculto inicial en el decoder."]},{"cell_type":"code","metadata":{"id":"PqN1jfzkE2O2"},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el encoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, src):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el encoder. Deberías        #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        \n","        #src = [src len, batch size]\n","        \n","        embedded = self.embedding(src)\n","        embedded = self.dropout(embedded)\n","\n","        #embedded = [src len, batch size, emb dim]\n","        \n","        outputs, hidden = self.rnn(embedded)\n","                \n","        #outputs = [src len, batch size, hid dim * num directions]\n","        #hidden = [n layers * num directions, batch size, hid dim]\n","        \n","        #hidden tiene las variables ocultas apiladas [forward_1, backward_1, forward_2, backward_2, ...]\n","        #los outputs provienen siempre de la última capa\n","        \n","        #hidden [-2, :, : ] es la última capa de las RNN hacia adelante\n","        #hidden [-1, :, : ] es la última capa de las RNN hacia atrás\n","        \n","        #el estado oculto inicial del decoder es el estado oculto final del encoder\n","        #tanto de las capas hacia adelante como hacia atrás que se pasan por una capa densa\n","\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], \n","                                               hidden[-1,:,:]), dim = 1)))\n","        \n","        #outputs = [src len, batch size, enc hid dim * 2]\n","        #hidden = [batch size, dec hid dim]\n","        \n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        return outputs, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k10kkK_4E2O3"},"source":["### Atención\n","\n","Lo siguiente es la capa de atención. Esto tomará el estado oculto anterior del decoder, $ s_{t-1} $, y todos los estados ocultos hacia adelante y hacia atrás del encoder apilados, $ H $. La capa generará un vector de atención, $ a_t $, que tiene la misma longitud de la oración origen, donde cada elemento está entre 0 y 1 y el vector completo suma 1.\n","\n","Intuitivamente, esta capa toma lo que hemos decodificado hasta ahora, $ s_{t-1} $, y todo lo que hemos codificado, $ H $, para producir un vector, $ a_t $, que representa a qué palabras en la oración origen debemos prestar la mayor atención para predecir correctamente la siguiente palabra a decodificar, $ \\hat {y}_{t + 1} $.\n","\n","Primero, calculamos la *energía* entre el estado oculto anterior del decoder y los estados ocultos del encoder. Como los estados ocultos de nuestro encoder son una secuencia de $ T $ tensores , y nuestro estado oculto anterior del decoder es un solo tensor, lo primero que hacemos es `repetir` el estado oculto anterior del decoder $ T $ veces. Luego calculamos la energía, $ E_t $, entre ellos concateándolos y pasándolos a través de una capa lineal (`attn`) y una función de activación $ \\tanh $. \n","\n","$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$ \n","\n","Esto se puede considerar como un cálculo de qué tan bien cada estado oculto del encoder \"coincide\" con el estado oculto anterior del decoder.\n","\n","Actualmente tenemos un tensor **[dimensión de variables ocultas del decoder, longitud de oración de origen]** para cada ejemplo del lote. Queremos que sea **[longitud de oración de origen]** para cada ejemplo en el lote, ya que la atención debe estar sobre la longitud de la oración de origen. Esto se logra multiplicando la `energía` por un tensor **[1, dimensión de variables ocultas del decoder]**, $ v $.\n","\n","$$\\hat{a}_t = v E_t$$\n","\n","Podemos pensar en $ v $ como los pesos de una suma ponderada de la energía en todos los estados ocultos del encoder. Estos pesos nos dicen cuánto debemos prestar atención a cada token en la secuencia de origen. Los parámetros de $ v $ se inicializan aleatoriamente, pero se aprenden con el resto del modelo mediante backpropagation. Tenga en cuenta que $ v $ no depende del tiempo y que se usa el mismo $ v $ para cada paso de tiempo de la decodificación. Implementamos $ v $ como una capa lineal sin sesgo.\n","\n","Finalmente, nos aseguramos de que el vector de atención se ajuste a las restricciones de tener todos los elementos entre 0 y 1 y el vector sumando 1 pasándolo a través de una capa $ \\text{softmax} $.\n","\n","$$a_t = \\text{softmax}(\\hat{a_t})$$\n","\n","¡Esto nos da la atención sobre la oración original!\n","\n","Gráficamente, esto se parece a lo siguiente. Esto es para calcular el primer vector de atención, donde $ s_{t-1} = s_0 = z $. Los bloques verde/verde azulado representan los estados ocultos de los RNN hacia adelante y hacia atrás, y el cálculo de atención se realiza dentro del bloque rosa.\n","\n","![](https://i.imgur.com/epVe8kC.png)"]},{"cell_type":"code","metadata":{"id":"uaqqIq9DE2O3"},"source":["class Attention(nn.Module):\n","    def __init__(self, enc_hid_dim, dec_hid_dim):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para la capa de atención con la#\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n","        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n","\n","    def forward(self, hidden, encoder_outputs):\n","        ########################################################################\n","        # TODO: Implementá la función forward para la capa de atención. Deberías\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        \n","        #hidden = [batch size, dec hid dim]\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","        batch_size = encoder_outputs.shape[1]\n","        src_len = encoder_outputs.shape[0]\n","        \n","        #repetí el estado oculto del decoder src_len veces\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","        #hidden = [batch size, src len, dec hid dim]\n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n","        \n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs),\n","                                                dim = 2))) \n","        \n","        #energy = [batch size, src len, dec hid dim]\n","\n","        attention = self.v(energy).squeeze(2)\n","        \n","        #attention= [batch size, src len]\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","        return F.softmax(attention, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJTGmc5TE2O4"},"source":["### Decoder\n","\n","Lo siguiente es el decoder.\n","\n","El decoder contiene la capa de atención, `attention`, que toma el estado oculto anterior, $ s_{t-1} $, todos los estados ocultos del encoder, $ H $, y devuelve el vector de atención, $ a_t $.\n","\n","Luego usamos este vector de atención para crear un vector origen pesado, $ w_t $, denotado por `weighted`, que es una suma ponderada de los estados ocultos del encoder, $ H $, usando $ a_t $ como pesos.\n","\n","$$w_t = a_t H$$\n","\n","El embedding de la palabra de entrada, $ d (y_t) $, el vector origen pesado, $ w_t $, y el estado oculto anterior del decoder, $ s_ {t-1} $, se pasan al decoder RNN, con $ d( y_t) $ y $ w_t $ concatenados.\n","\n","$$s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})$$\n","\n","Luego pasamos $ d (y_t) $, $ w_t $ y $ s_t $ a través de la capa lineal, $ f $, para hacer una predicción de la siguiente palabra en la oración objetivo, $ \\hat{y}_{t + 1}$. Esto se hace concatenándolos todos juntos.\n","\n","$$\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)$$\n","\n","La siguiente imagen muestra la decodificación de la primera palabra en una traducción de ejemplo.\n","\n","![](https://i.imgur.com/bqJ2DU1.png)\n","\n","Los bloques verde/verde azulado muestran los RNN del encoder hacia adelante/atrás que generan $ H $, el bloque rojo muestra el vector de contexto, $ z = h_T = \\tanh (g (h ^ \\rightarrow_T, h ^ \\leftarrow_T)) = \\tanh(g (z ^ \\rightarrow, z ^ \\leftarrow)) = s_0 $, el bloque azul muestra el decoder RNN que genera $ s_t $, el bloque púrpura muestra la capa lineal, $ f $, que genera $ \\hat{y}_{t + 1} $ y el bloque naranja muestra el cálculo de la suma pesada sobre $ H $ por $ a_t $ y genera $ w_t $. No se muestra el cálculo de $ a_t $."]},{"cell_type":"code","metadata":{"id":"QI9nQlyWE2O4"},"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el decoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.output_dim = output_dim\n","        self.attention = attention\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","        self.fc = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, input, hidden, encoder_outputs):\n","             \n","        #input = [batch size]\n","        #hidden = [batch size, dec hid dim]\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","        \n","        input = input.unsqueeze(0)\n","        \n","        #input = [1, batch size]\n","        \n","        embedded = self.embedding(input)\n","        embedded = self.dropout(embedded)\n","\n","        #embedded = [1, batch size, emb dim]\n","        \n","        a = self.attention(hidden, encoder_outputs)\n","                \n","        #a = [batch size, src len]\n","        \n","        a = a.unsqueeze(1)\n","        \n","        #a = [batch size, 1, src len]\n","        \n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","        \n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n","        \n","        weighted = torch.bmm(a, encoder_outputs)\n","        \n","        #weighted = [batch size, 1, enc hid dim * 2]\n","        \n","        weighted = weighted.permute(1, 0, 2)\n","        \n","        #weighted = [1, batch size, enc hid dim * 2]\n","        \n","        rnn_input = torch.cat((embedded, weighted), dim = 2)\n","        \n","        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n","            \n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n","        \n","        #output = [seq len, batch size, dec hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, dec hid dim]\n","        \n","        #seq len, n layers y n directions valdrá siempre 1 en el decoder, por lo tanto:\n","        #output = [1, batch size, dec hid dim]\n","        #hidden = [1, batch size, dec hid dim]\n","        #esto tmbién significa que output == hidden\n","        assert (output == hidden).all()\n","        \n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted = weighted.squeeze(0)\n","        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))\n","        \n","        #prediction = [batch size, output dim]\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","        return prediction, hidden.squeeze(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWb-mLtOE2O5"},"source":["### Seq2Seq\n","\n","Este es el primer modelo en el que no es necesario que el encoder RNN y el decoder RNN tengan las mismas dimensiones ocultas, sin embargo, el encoder tiene que ser bidireccional. Este requisito puede eliminarse cambiando todas las apariciones de `enc_dim * 2` a` enc_dim * 2 si encoder_is_bidirectional else enc_dim`.\n","\n","Este encapsulador seq2seq es similar a los dos últimos. La única diferencia es que el `encoder` devuelve tanto el estado oculto final (que es el estado oculto final de los RNN del encoder hacia adelante y hacia atrás que pasan a través de una capa lineal) para ser utilizado como el estado oculto inicial para el decoder,  como también todos los estados ocultos (que son los estados ocultos hacia adelante y hacia atrás apilados uno encima del otro). También debemos asegurarnos de que `hidden` y` encoder_outputs` se pasen al decoder.\n","\n","Repasando brevemente todos los pasos:\n","- el tensor de `outputs` se crea para contener todas las predicciones, $ \\hat {Y} $\n","- la secuencia de origen, $ X $, se alimenta al encoder para recibir $ z $ y $ H $\n","- el estado oculto inicial del decoder se establece en el vector `context`, $ s_0 = z = h_T $\n","- usamos un lote de tokens `<sos>` como la primera `input`, $ y_1 $\n","- luego decodificamos dentro de un bucle:\n","   - insertando el token de entrada $ y_t $, estado oculto anterior, $ s_ {t-1} $, y todas las salidas del encoder, $ H $, en el decoder\n","   - recibiendo una predicción, $ \\hat{y}_{t + 1} $, y un nuevo estado oculto, $ s_t $\n","   - luego decidiendo si vamos a hacer forzamiento del maestro o no, estableciendo la siguiente entrada según corresponda"]},{"cell_type":"code","metadata":{"id":"6X_ZM67HE2O6"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el modelo seq2seq con la  #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio es la probabilidad con la que se usa el forzamiento del maestro\n","        #e.g. si teacher_forcing_ratio es 0.75 usamos las etiquetas el 75% del tiempo\n","        \n","        batch_size = src.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor para almacenar las salidas del decoder \n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #encoder_outputs son todos los estados ocultos de la secuencia de origen, hacia adelante y hacia atrás\n","        #hidden son los estados ocultos finales haci aadelante y hacia atrás pasados a través de una capa densa\n","        encoder_outputs, hidden = self.encoder(src)\n","                \n","        #la primera entrada al decoder es el token <sos>\n","        input = trg[0,:]\n","        \n","        for t in range(1, trg_len):\n","            ########################################################################\n","            # TODO: Implementá el bucle de decodificación dentro de la función     #\n","            # forward para el modelo seq2seq. Deberías usar las capas que definiste#     \n","            # en __init__ y especificar la conectividad de dichas capas            #\n","            ########################################################################\n","            # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","                \n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[t] if teacher_force else top1\n","\n","            # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","            ########################################################################\n","            #                          FINAL DE TU CÓDIGO                          #       \n","            ########################################################################\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLwkHRziE2O6"},"source":["## Entrenamiento del modelo Seq2Seq\n","\n","El resto de esta sección es muy similar a la anterior.\n","\n","Inicializamos nuestros parámetros, encoder, decoder y modelo seq2seq (colocándolo en la GPU si tenemos una)."]},{"cell_type":"code","metadata":{"id":"IFLUg-deE2O7"},"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 512\n","DEC_HID_DIM = 512\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vxDcpfX0E2O7"},"source":["Usamos una versión simplificada del esquema de inicialización de peso utilizado en el paper. Aquí, inicializaremos todos los sesgos a cero y todos los pesos con $ \\mathcal {N} (0, 0.01) $."]},{"cell_type":"code","metadata":{"id":"h_5vHQ6DE2O7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623160775151,"user_tz":180,"elapsed":306,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"058f3bc5-096e-471a-fd52-5f304229ca29"},"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","            \n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7855, 256)\n","    (rnn): GRU(256, 512, bidirectional=True)\n","    (fc): Linear(in_features=1024, out_features=512, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=1536, out_features=512, bias=True)\n","      (v): Linear(in_features=512, out_features=1, bias=False)\n","    )\n","    (embedding): Embedding(5893, 256)\n","    (rnn): GRU(1280, 512)\n","    (fc): Linear(in_features=1792, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"mpf25ipPE2O7"},"source":["Calculamos el número de parámetros. Obtenemos un aumento de casi un 50% en la cantidad de parámetros con respecto al último modelo."]},{"cell_type":"code","metadata":{"id":"YjcpMAgsE2O7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623160778706,"user_tz":180,"elapsed":404,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"b9635154-656f-47b7-a888-272c09dee78c"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 20,518,917 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v25G6DL3E2O8"},"source":["Creamos un optimizador"]},{"cell_type":"code","metadata":{"id":"_k40uzxNE2O8"},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VBkp1me1E2O8"},"source":["Inicializamos la función de pérdida.\n"]},{"cell_type":"code","metadata":{"id":"_vAqWxFNE2O9"},"source":["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4MWjFirE2O9"},"source":["Y creamos el loop de entrenamiento."]},{"cell_type":"code","metadata":{"id":"jT0vRN4AE2O9"},"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","        \n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","        \n","        output_dim = output.shape[-1]\n","        \n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","        \n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ju9_eD9OE2O9"},"source":["...y el loop de evaluación, recordando configurar el modelo en modo `eval` y desactivar el forzamiento del maestro."]},{"cell_type":"code","metadata":{"id":"VP7-zTELE2O-"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #desactiva el forzamiento del maestro\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYtD26LPE2O-"},"source":["Finalmente, definimos una función de temporización."]},{"cell_type":"code","metadata":{"id":"PSZj7zMCE2O-"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dqWE8aUE2O-"},"source":["Luego, entrenamos nuestro modelo, guardando los parámetros que nos den la mejor pérdida de validación."]},{"cell_type":"code","metadata":{"id":"xuUNBPwVE2O_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623161838728,"user_tz":180,"elapsed":826903,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"6a67018c-6971-4893-b447-592eb102113d"},"source":["N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut3-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 1m 22s\n","\tTrain Loss: 5.045 | Train PPL: 155.301\n","\t Val. Loss: 4.850 |  Val. PPL: 127.774\n","Epoch: 02 | Time: 1m 22s\n","\tTrain Loss: 4.093 | Train PPL:  59.925\n","\t Val. Loss: 4.110 |  Val. PPL:  60.966\n","Epoch: 03 | Time: 1m 22s\n","\tTrain Loss: 3.347 | Train PPL:  28.418\n","\t Val. Loss: 3.614 |  Val. PPL:  37.129\n","Epoch: 04 | Time: 1m 23s\n","\tTrain Loss: 2.819 | Train PPL:  16.759\n","\t Val. Loss: 3.368 |  Val. PPL:  29.024\n","Epoch: 05 | Time: 1m 22s\n","\tTrain Loss: 2.460 | Train PPL:  11.709\n","\t Val. Loss: 3.270 |  Val. PPL:  26.311\n","Epoch: 06 | Time: 1m 23s\n","\tTrain Loss: 2.161 | Train PPL:   8.681\n","\t Val. Loss: 3.269 |  Val. PPL:  26.291\n","Epoch: 07 | Time: 1m 22s\n","\tTrain Loss: 1.922 | Train PPL:   6.835\n","\t Val. Loss: 3.252 |  Val. PPL:  25.846\n","Epoch: 08 | Time: 1m 22s\n","\tTrain Loss: 1.717 | Train PPL:   5.570\n","\t Val. Loss: 3.315 |  Val. PPL:  27.533\n","Epoch: 09 | Time: 1m 22s\n","\tTrain Loss: 1.559 | Train PPL:   4.756\n","\t Val. Loss: 3.365 |  Val. PPL:  28.938\n","Epoch: 10 | Time: 1m 21s\n","\tTrain Loss: 1.413 | Train PPL:   4.109\n","\t Val. Loss: 3.479 |  Val. PPL:  32.433\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5U4K889QE2PA"},"source":["Finalmente, probamos el modelo en el conjunto de prueba usando estos \"mejores\" parámetros."]},{"cell_type":"code","metadata":{"id":"fkqE3glnE2PB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623161848687,"user_tz":180,"elapsed":1353,"user":{"displayName":"Exequiel Santucho","photoUrl":"","userId":"10709628178102361168"}},"outputId":"6f7d7c62-fe25-4a41-acf8-36a534b0e14d"},"source":["model.load_state_dict(torch.load('tut3-model.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 3.292 | Test PPL:  26.897 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lmv06pzaE2PB"},"source":["Hemos mejorado el modelo anterior, pero esto tuvo el costo de duplicar el tiempo de entrenamiento."]}]}