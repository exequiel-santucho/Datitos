{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":460,"status":"ok","timestamp":1623251013194,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"t3p9-fDtzWaA"},"outputs":[],"source":["#@title Aprendizaje Profundo | Otoño 2021 by Datitos{display-mode: \"form\" }\n","#@markdown ![71335171.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAIAAAAiOjnJAAACwElEQVR4nOzdMY7iQBBA0WU197/FnJNNJ/FqWvLHZfd7McIGfVVQos3X+/3+A2f7e/UN8EzCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBJfV9/A/7xer6XX3/1/gZ70eU0sEsIiISwSwiIhLBLCIiEsEiP2WEf7m9U9zVnvU9vh85pYJIRFQlgkhEVCWCSERUJYJEbssc5ytL+5at8zec9UM7FICIuEsEgIi4SwSAiLhLBIPGqPdWR1v1VfdwcmFglhkRAWCWGREBYJYZEQFokt9lhHdt4z1UwsEsIiISwSwiIhLBLCIiEsEh/dY+18zq4w7RzlTyYWCWGREBYJYZEQFglhkRAWiS1+jzVhr/Mbd7nP3zCxSAiLhLBICIuEsEgIi4SwSDxqj1U/72r1ulc9l2sCE4uEsEgIi4SwSAiLhLBICIvE7D3W9/fSy6/63dLqdZfvc/F7mMDEIiEsEsIiISwSwiIhLBLCIjF7j3WSs87rTXufyUwsEsIiISwSwiIhLBLCIiEsEq8Ju5N6r3OXc3xP2oeZWCSERUJYJIRFQlgkhEVCWCRG7LGOTN7TfNIdvwcTi4SwSAiLhLBICIuEsEgIi8Toc4U7Pyf9p8n7qiMmFglhkRAWCWGREBYJYZEQFonRe6wjd9zr7MbEIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIi8S8AAP//HtRtH09JwIEAAAAASUVORK5CYII=)\n","#El siguiente notebook fue traducido por Pablo Marinozi como el séptimo trabajo práctico correspondiente a la versión de Otoño del 2021 del curso Aprendizaje Profundo organizado por Datitos\n","#El tutorial original fue diseñado por Ben Trevett y fue publicado en su github https://github.com/bentrevett\n","#Para mayor información consultar https://datitos.github.io/curso-aprendizaje-profundo/#calendario"]},{"cell_type":"markdown","metadata":{"id":"eLiZyyaIzk71"},"source":["#Trabajo Práctico N° 8: Aprendizaje Secuencia a Secuencia con Transformers"]},{"cell_type":"markdown","metadata":{"id":"COKr3uj3A6F8"},"source":["## Todo lo que necesitas es prestar atención\n","\n","En este práctico implementaremos una (versión ligeramente modificada) del modelo Transformer del paper [Attention is All You Need](https://arxiv.org/abs/1706.03762). Todas las imágenes de este notebook se tomaron del paper de Transformer. Para obtener más información sobre el Transformer, [mirá](https://www.mihaileric.com/posts/transformers-attention-in-disguise/) [estos](https://jalammar.github.io/illustrated-transformer/) [tres](http://nlp.seas.harvard.edu/2018/04/03/attention.html) artículos.\n","\n","![](https://i.imgur.com/umPkXYX.png)\n","\n","## Introducción\n","\n","Los Transformers no utilizan ningún tipo de recurrencia. En cambio, el modelo está compuesto por capas lineales, mecanismos de atención y normalización.\n","\n","Al momento de creación de este práctico (Junio del 2021), los Transformers son la arquitectura dominante en NLP, se utilizan para lograr resultados del estado del arte en muchas tareas y parece que se seguirán utilizando en un futuro próximo.\n","\n","La variante de Transformer más popular es [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) y sus versiones pre-entrenadas se utilizan comúnmente para reemplazar las capas de incrustación, si no más, en los modelos de NLP.\n","\n","Las diferencias entre la implementación en este notebook y el paper son:\n","- utilizamos una codificación posicional aprendida en lugar de una estática\n","- utilizamos el optimizador Adam estándar con una tasa de aprendizaje estático en lugar de uno con pasos de calentamiento y enfriamiento\n","- no usamos suavizado de etiquetas.\n","\n","Realizamos todos estos cambios ya que siguen de cerca la configuración de BERT y la mayoría de las variantes de Transformer utilizan una configuración similar."]},{"cell_type":"markdown","metadata":{"id":"QE2A3ZU2A6GB"},"source":["## Preparando los datos\n","\n","Como siempre, importemos todos los módulos necesarios y establezcamos las semillas aleatorias para la reproducibilidad."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1113,"status":"ok","timestamp":1623251014300,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"HPmsvcUrA6GD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchtext\n","from torchtext.legacy.datasets import Multi30k\n","from torchtext.legacy.data import Field, BucketIterator, Example, Dataset # Agrego Example y Dataset para poder guardar archivos en local\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import spacy\n","import numpy as np\n","\n","import random\n","import math\n","import time"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1623251014311,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"J4yOrCRVA6GF"},"outputs":[],"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"E54auqYiA6GG"},"source":["Luego vamos a crear nuestros tokenizadores como antes."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7561,"status":"ok","timestamp":1623251021849,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"icr5U96VRUTO","outputId":"94db7045-96e9-479d-bab3-5fedf13c5784"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy\u003e=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: blis\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: catalogue\u003c1.1.0,\u003e=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (57.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: srsly\u003c1.1.0,\u003e=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: importlib-metadata\u003e=0.20; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (4.0.1)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20; python_version \u003c \"3.8\"-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.4.1)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20; python_version \u003c \"3.8\"-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.7.4.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy\u003e=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: srsly\u003c1.1.0,\u003e=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue\u003c1.1.0,\u003e=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (57.0.0)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: blis\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: importlib-metadata\u003e=0.20; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (4.0.1)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20; python_version \u003c \"3.8\"-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20; python_version \u003c \"3.8\"-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.4.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_sm')\n"]}],"source":["# Descargo los modelos de spaCy ejecutando la siguiente celda\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm\n","\n","# Después de descargar los modelos, tenemos que reiniciar el entorno de ejecución y correr la siguiente celda."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2724,"status":"ok","timestamp":1623251026773,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"yRA5-q_RA6GH"},"outputs":[],"source":["spacy_de = spacy.load('de_core_news_sm')\n","spacy_en = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1623251030023,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"176NWaS8A6GH"},"outputs":[],"source":["def tokenize_de(text):\n","    \"\"\"\n","    Tokenizes German text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_de.tokenizer(text)]\n","\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokenizes English text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"]},{"cell_type":"markdown","metadata":{"id":"Y7Q4HstrA6GI"},"source":["Nuestros Fields son los mismos que en el trabajo práctico anterior. El modelo espera que los datos se introduzcan primero con la dimensión del lote, por lo que usamos `batch_first = True`. "]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1623251035448,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"cRmw4QN9A6GJ"},"outputs":[],"source":["SRC = Field(tokenize = tokenize_de, \n","            init_token = '\u003csos\u003e', \n","            eos_token = '\u003ceos\u003e', \n","            lower = True, \n","            batch_first = True)\n","\n","TRG = Field(tokenize = tokenize_en, \n","            init_token = '\u003csos\u003e', \n","            eos_token = '\u003ceos\u003e', \n","            lower = True, \n","            batch_first = True)"]},{"cell_type":"markdown","metadata":{"id":"npwl1QT2A6GM"},"source":["Luego cargamos el dataset Multi30k y construimos el vocabulario."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1545456,"status":"ok","timestamp":1623252583190,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"9-gzS5XwA6GM","outputId":"5322466c-665a-4472-ffab-621fbe3b4b6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading training.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00\u003c00:00, 1.60MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading validation.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00\u003c00:00, 240kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading mmt_task1_test2016.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00\u003c00:00, 235kB/s]\n"]}],"source":["train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n","                                                    fields = (SRC, TRG))"]},{"cell_type":"markdown","metadata":{"id":"jUp-F4JZYNmO"},"source":["Esta etapa de tokenización puede tardar alrededor de 20 minutos en ejecutarse. Para evitar tener que esperar tanto tiempo cada vez que generemos una sesión en Colab, vamos a guardar el dataset tokenizado en archivos.\n","\n","**Nota: Los archivos se borrarán cuando se desconecte la sesión. Asegurate de descargarlos a tu máquina o guardarlos en tu drive.** \n","\n","DESCOMENTAR DESDE CELDA SIGUIENTE HASTA PRÓXIMA CELDA INDICADA"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1071,"status":"aborted","timestamp":1623250988837,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"wCiEEq2SYQif"},"outputs":[],"source":["# import json\n","# def save_examples(dataset, savepath):\n","#     with open(savepath, 'w') as f:\n","#         # Save num. elements (not really need it)\n","#         f.write(json.dumps(len(dataset.examples)))  # Write examples length\n","#         f.write(\"\\n\")\n","\n","#         # Save elements\n","#         for pair in dataset.examples:\n","#             data = [pair.src, pair.trg]\n","#             f.write(json.dumps(data))  # Write samples\n","#             f.write(\"\\n\")\n","\n","\n","# def load_examples(filename):\n","#     examples = []\n","#     with open(filename, 'r') as f:\n","#         # Read num. elements (not really need it)\n","#         total = json.loads(f.readline())\n","\n","#         # Save elements\n","#         for i in range(total):\n","#             line = f.readline()\n","#             example = json.loads(line)\n","#             # example = data.Example().fromlist(example, fields)  # Create Example obj. (you can do it here or later)\n","#             examples.append(example)\n","#     return examples"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1071,"status":"aborted","timestamp":1623250988838,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"GNQOo10zYUSo"},"outputs":[],"source":["# #Guarda los datasets tokenizados en disco # No uso esto\n","# # save_examples(train_data,\"train.json\")\n","# # save_examples(valid_data,\"valid.json\")\n","# # save_examples(test_data,\"test.json\")\n","\n","# # ---\n","# # Guardo los datasets tokenizados en carpeta de Google Drive\n","# # Monto Google Drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# # Al continuar trabajo en otro día o si se finaliza el entorno de ejecución,\n","# # ejecutar esta celda y la siguiente. Si tira error, ejecutar las anteriores\n","# # para cargar las variables que no estén."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1072,"status":"aborted","timestamp":1623250988839,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"gH22DSS7YdS7"},"outputs":[],"source":["# save_examples(train_data,\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP8/tmp/train.json\")\n","# save_examples(valid_data,\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP8/tmp/valid.json\")\n","# save_examples(test_data,\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP8/tmp/test.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1073,"status":"aborted","timestamp":1623250988840,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"HjyhmjTxYnDL"},"outputs":[],"source":["# #Carga los datasets tokenizados desde los archivos\n","# SRC = Field(tokenize = tokenize_de, \n","#             init_token = '\u003csos\u003e', \n","#             eos_token = '\u003ceos\u003e', \n","#             lower = True)\n","\n","# TRG = Field(tokenize = tokenize_en, \n","#             init_token = '\u003csos\u003e', \n","#             eos_token = '\u003ceos\u003e', \n","#             lower = True)\n","# fields = [('src', SRC), ('trg', TRG)]\n","\n","# # examples = load_examples(\"train.json\") # cargo desde Google Drive\n","# examples = load_examples(\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP8/tmp/train.json\")\n","# examples = [Example().fromlist(d, fields) for d in examples]\n","# train_data = Dataset(examples, fields)\n","\n","# # examples = load_examples(\"valid.json\") # cargo desde Google Drive\n","# examples = load_examples(\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP8/tmp/valid.json\")\n","# examples = [Example().fromlist(d, fields) for d in examples]\n","# valid_data = Dataset(examples, fields)\n","\n","# # examples = load_examples(\"test.json\") # cargo desde Google Drive\n","# examples = load_examples(\"/content/drive/MyDrive/Colab Notebooks/DATITOS 2021/TP8/tmp/test.json\")\n","# examples = [Example().fromlist(d, fields) for d in examples]\n","# test_data = Dataset(examples, fields)"]},{"cell_type":"markdown","metadata":{"id":"YiE62Fr1_nw0"},"source":["DESCOMENTAR LO ANTERIOR PARA ACTIVAR GUARDADO EN DISCO LOCAL"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":694,"status":"ok","timestamp":1623252591333,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"xrTvznONA6GN"},"outputs":[],"source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)"]},{"cell_type":"markdown","metadata":{"id":"UVyu8GcYA6GN"},"source":["Finalmente, definimos el `device` y el iterador de datos."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":259,"status":"ok","timestamp":1623252593063,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"r0il91gNA6GO"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1623252594005,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"TGLmkH8sA6GO"},"outputs":[],"source":["BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","     batch_size = BATCH_SIZE,\n","     device = device)"]},{"cell_type":"markdown","metadata":{"id":"nBMupg3m2tRA"},"source":["## Capas Necesarias para el Modelo\n","\n","Antes de armar el modelo, vamos a explicar algunas capas introducidas en el paper que forman una parte esencial tanto dentro del encoder como del decoder."]},{"cell_type":"markdown","metadata":{"id":"h6pYQrt9A6GR"},"source":["### Capa de atención de múltiples cabezales\n","\n","Uno de los conceptos clave y novedosos introducidos por el documento Transformer es la *capa de atención de múltiples cabezales*. \n","\n","![](https://i.imgur.com/CwYzPgi.png)\n","\n","La atención se puede considerar como ***consultas (queries)***, ***claves (keys)*** y ***valores (values)***, donde la consulta se usa junto a la clave para obtener un vector de atención (generalmente el resultado de una operación *softmax* y tiene todos los valores entre 0 y 1 que suma a 1) que luego se usa para obtener una suma ponderada de los valores.\n","\n","El Transformer utiliza ***atención de producto punto escalada***, donde la consulta y la clave se combinan tomando el producto punto entre ellas, luego aplicando softmax y escalando por $d_k$ antes de finalmente multiplicar por el valor. La constante $ d_k $ es la ***dimensión de la cabeza***, `head_dim`, que explicaremos con más detalle en breve.\n","\n","$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n","\n","Esto es similar a la ***atención de producto punto*** estándar pero escalada por $ d_k $, que según el documento se usa para evitar que los resultados de los productos punto se hagan demasiado grandes, y por lo tanto los gradientes se vuelvan demasiado pequeños.\n","\n","Sin embargo, la atención de producto punto escalada no se aplica simplemente a las consultas, claves y valores. En lugar de realizar una aplicación de atención única, las consultas, claves y valores tienen su `hid_dim` dividido en ***$ h $ cabezas*** y la atención de producto punto escalada se calcula sobre todas las cabezas en paralelo. Esto significa que en lugar de prestar atención a un concepto por aplicación de atención, prestamos atención a $ h $ conceptos. Luego, volvemos a combinar las cabezas en su forma `hid_dim`.\n","\n","$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n","\n","$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n","\n","$ W ^ O $ es la capa densa aplicada al final de la capa de atención de múltiples cabezales, que llamaremos en el código `fc_o`. $ W ^ Q, W ^ K, W ^ V $ son las capas densas que en el código llamaremos `fc_q`,` fc_k` y `fc_v`.\n","\n","Recorriendo el código del módulo, primero calculamos $ QW ^ Q $, $ KW ^ K $ y $ VW ^ V $ con las capas lineales, `fc_q`, ` fc_k` y `fc_v`, para darnos ` Q`, ` K` y `V`. A continuación, dividimos el `hid_dim` de la consulta, la clave y el valor en ` n_heads` usando `.view()` y los permutamos correctamente para que se puedan multiplicar juntos. Luego calculamos la `energía` (la atención no normalizada) multiplicando ` Q` y `K` y escalando por la raíz cuadrada de` head_dim`, que se calcula como `hid_dim // n_heads`. Luego enmascaramos la energía para que no prestemos atención a ningún elemento de la secuencia que no deberíamos, luego aplicamos el softmax y dropout. A continuación, aplicamos la atención a los valores de los cabezales, `V`, antes de combinar los resultados para las ` n_heads` . Finalmente, multiplicamos este $ W ^ O $, representado por `fc_o`.\n","\n","Tenga en cuenta que en nuestra implementación las longitudes de las claves y los valores son siempre los mismos, por lo tanto, cuando la matriz multiplica la salida del softmax, `attention`, con` V` siempre tendremos tamaños de dimensión válidos para la multiplicación de matrices. Esta multiplicación se lleva a cabo usando `torch.matmul` que, cuando ambos tensores son \u003e2-dimensionales, hace una multiplicación matricial por lotes sobre las dos últimas dimensiones de cada tensor. Esto será una multiplicación de matriz por lotes con formas **[query len, key len] x [value len, head dim]**   sobre el tamaño de lote y cada cabezal que proporciona un resultado con forma **[batch size, n heads, query len, head dim]** .\n","\n","Algo que parece extraño al principio es que el dropout se aplica directamente a la atención. Esto significa que lo más probable es que nuestro vector de atención no sume 1 o que prestemos toda la atención a un solo token que puede establecerse en 0 por el dropout. Estos problemas nunca se explican, ni siquiera se mencionan, en el paper, sin embargo, es lo que se hace en la [implementación oficial](https://github.com/tensorflow/tensor2tensor/) y todas las implementaciones de Transformer desde entonces, [incluido BERT](https://github.com/google-research/bert/)."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1623252601558,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"ipH7sT_MA6GR"},"outputs":[],"source":["class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","        \n","    def forward(self, query, key, value, mask = None):\n","        \n","        batch_size = query.shape[0]\n","        \n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","                \n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        \n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","                \n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        \n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","                \n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        \n","        #energy = [batch size, n heads, query len, key len]\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","        \n","        attention = torch.softmax(energy, dim = -1)\n","                \n","        #attention = [batch size, n heads, query len, key len]\n","                \n","        x = torch.matmul(self.dropout(attention), V)\n","        \n","        #x = [batch size, n heads, query len, head dim]\n","        \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        \n","        #x = [batch size, query len, n heads, head dim]\n","        \n","        x = x.view(batch_size, -1, self.hid_dim)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        x = self.fc_o(x)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        return x, attention"]},{"cell_type":"markdown","metadata":{"id":"f3-95fx6A6GS"},"source":["### Capa Feed-Forward Posicional \n","\n","La otra capa presentada en el paper es la *capa feedforward posicional*. Esta capa es relativamente simple en comparación con la capa de atención de múltiples cabezales. La entrada se transforma de `hid_dim` a` pf_dim`, donde `pf_dim` suele ser mucho más grande que` hid_dim`. El Transformer original usó un `hid_dim` de 512 y un` pf_dim` de 2048. La función de activación de ReLU y el dropout se aplican antes de que se vuelva a transformar en una representación de `hid_dim`.\n","\n","BERT usa la función de activación [GELU](https://arxiv.org/abs/1606.08415), que se puede usar simplemente cambiando `torch.relu` por` F.gelu`. "]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":254,"status":"ok","timestamp":1623252607222,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"mJiTfQXzA6GS"},"outputs":[],"source":["class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        \n","        #x = [batch size, seq len, pf dim]\n","        \n","        x = self.fc_2(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x"]},{"cell_type":"markdown","metadata":{"id":"1YSm02hY2Tdi"},"source":["## Construyendo el modelo\n","\n","A continuación, crearemos el modelo. Al igual que en el trabajo práctico anterior, se compone de un *encoder* y un *decoder*, con el encoder *codificando* la oración de entrada/origen (en alemán) en un *vector de contexto* y el decoder luego *decodificando* este vector de contexto para generar nuestra oración de salida/objetivo (en inglés)."]},{"cell_type":"markdown","metadata":{"id":"859EdNC5A6GQ"},"source":["### Bloque Encoder \n","\n","Los bloques Encoder son donde se encuentra toda la \"magia\" del Encoder. \n","\n","![](https://i.imgur.com/adzmIfx.png)\n","\n","Primero pasamos la oración origen y su máscara a la *capa de atención de múltiples cabezales*, luego aplicamos dropout, una conexión residual y una capa de  [Normalización por capas](https://arxiv.org/abs/1607.06450 ). Luego lo pasamos a través de una capa feed-forward posicional y luego, nuevamente, aplicamos dropout, una conexión residual y  la capa de normalización por capas para obtener la salida de este bloque que se alimenta al siguiente bloque. Los parámetros no se comparten entre bloques.\n","\n","La capa de atención de múltiples cabezales es utilizada por el bloque encoder para prestar atención a la oración origen, es decir, está calculando y aplicando atención sobre sí misma en lugar de otra secuencia, por lo que la llamamos *auto-atención*.\n","\n","[Este](https://becominghuman.ai/all-about-normalization-6ea79e70894b) artículo entra en más detalles sobre la normalización de capas, pero la esencia es que normaliza los valores de las features, de manera que cada feature tiene una media de 0 y una desviación estándar de 1. Esto permite que las redes neuronales con una mayor número de capas, como el Transformer, puedan entrenar más fácilmente."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1623252615165,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"_uaJKmWJA6GQ"},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):        \n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el bloque encoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, src, src_mask):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el bloque encoder. Deberías #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","\n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, 1, 1, src len] \n","                \n","        self_src, _ = self.self_attention(src, src, src, src_mask)\n","        src = self.self_attn_layer_norm(src + self.dropout(self_src))\n","\n","        poswis_src = self.positionwise_feedforward(src)\n","        src = self.ff_layer_norm(src + self.dropout(poswis_src))\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        return src"]},{"cell_type":"markdown","metadata":{"id":"VX1WyENPA6GO"},"source":["### Encoder\n","El encoder del Transformer no intenta comprimir toda la oración fuente, $ X = (x_1, ..., x_n) $, en un solo vector de contexto, $ z $. En su lugar, produce una secuencia de vectores de contexto, $ Z = (z_1, ..., z_n) $. De esta manera, si nuestra secuencia de entrada tuviera 5 tokens de longitud, tendríamos $ Z = (z_1, z_2, z_3, z_4, z_5) $. ¿Por qué llamamos a esto una secuencia de vectores de contexto y no una secuencia de variables ocultas? Porque una variable oculta en el momento $ t $ en una RNN solo ha visto el token $ x_t $ y todos los tokens anteriores. Sin embargo, cada vector de contexto aquí ha visto todos los tokens en todas las posiciones dentro de la secuencia de entrada.\n","\n","![](https://i.imgur.com/6gOHngA.png)\n","\n","Primero, los tokens se pasan a través de una capa de embedding estándar. A continuación, como el modelo no tiene recurrencias, no tiene idea del orden de los tokens dentro de la secuencia. Resolvemos este problema usando una segunda capa de embedding llamada *capa de embedding posicional*. Esta es una capa de embedding estándar donde la entrada no es el token en sí, sino la posición del token dentro de la secuencia, comenzando con el primer token, el token `\u003csos\u003e` (inicio de secuencia), en la posición 0. El embedding posicional tiene un tamaño de \"vocabulario\" de 100, lo que significa que nuestro modelo puede aceptar oraciones de hasta 100 tokens de longitud. Esto se puede aumentar si queremos manejar oraciones más largas.\n","\n","La implementación del Transformer original del paper \"Attention is All You Need\" no aprende los embedding posicionales. En su lugar, utiliza una codificación posicional estática fija. Las arquitecturas modernas de Transformer, como BERT, usan embedding posicionales aprendibles en su lugar, por lo tanto, hemos decidido usarlos en este trabajo práctico. Consulte [esta](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) sección para leer más sobre la codificación posicional utilizada en el modelo Transformer original.\n","\n","A continuación, el token y los embeddings posicionales se suman elemento a elemento para obtener un vector que contiene información sobre el token y también su posición en la secuencia. Sin embargo, antes de sumarlos, los embeddings de tokens se multiplican por un factor de escala que es $ \\sqrt{d_{modelo}} $, donde $ d_{modelo} $ es la dimensión de las capas ocultas, `hid_dim`. Esto supuestamente reduce la variación en los embeddings y el modelo es difícil de entrenar de manera confiable sin este factor de escala. A continuación, se aplica  dropout a los embeddings combinados.\n","\n","Los embeddings combinados se pasan a través de $ N $ de los ***bloques encoder***, definidos en la sección anterior, para obtener $ Z $, que luego se envía y puede ser utilizado por el decoder.\n","\n","La máscara origen, `src_mask`, tiene simplemente la misma forma que la oración origen pero tiene un valor de 1 cuando el token en la oración origen no es un token` \u003cpad\u003e `y 0 cuando sí lo es. Esto se usa en las capas del encoder para enmascarar los mecanismos de atención de múltiples cabezales, que se usan para calcular y aplicar atención sobre la oración origen, para que el modelo no preste atención a los tokens `\u003cpad\u003e`, que no contienen información útil."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1623252621709,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"BKzNH8U6A6GP"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout,\n","                 device, max_length = 100):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el encoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.device = device\n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        self.layers = nn.ModuleList([EncoderBlock(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim,\n","                                                  dropout, \n","                                                  device) \n","                                     for _ in range(n_layers)])\n","        self.dropout = nn.Dropout(dropout)\n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, src, src_mask):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el encoder. Deberías        #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","\n","        #src = [batch size, src len]\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","        #pos = [batch size, src len]\n","        \n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","            \n","        #src = [batch size, src len, hid dim]\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################        \n","            \n","        return src"]},{"cell_type":"markdown","metadata":{"id":"0J5FhLmbA6GU"},"source":["### Bloque Decoder\n","\n","El bloque decoder es similar al bloque encoder excepto que ahora tiene dos capas de atención de múltiples cabezales, `self_attention` y `encoder_attention`.\n","\n","![](https://i.imgur.com/DjsYbBi.png)\n","\n","La primera realiza la auto-atención, como en el encoder, utilizando la representación del decoder hasta el momento como consulta, clave y valor. A esto le sigue el dropout, la conexión residual y la normalización por capas. Esta capa `self_attention` usa la máscara de secuencia objetivo,` trg_mask`, para evitar que el decoder \"haga trampas\" prestando atención a los tokens que están \"por delante\" del que está procesando actualmente, ya que procesa en paralelo todos los tokens en la oración objetivo.\n","\n","La segunda es la manera en que realmente alimentamos la oración origen codificada, `enc_src`, en nuestro decoder. En esta capa de atención de múltiples cabezales, las consultas son las representaciones del decoder hasta el momento y las claves y valores son las representaciones del encoder. Aquí, la máscara de origen, `src_mask` se usa para evitar que la capa de atención de múltiples cabezales preste atención a los tokens ` \u003cpad\u003e ` dentro de la oración de origen. A esto le siguen la capa de dropout, la conexión residual y la capa de normalización por capas. \n","\n","Finalmente, pasamos esto a través de la capa feed-forward posicional y otra secuencia de dropout, conexión residual y normalización por capa.\n","\n","El bloque decoder no está introduciendo ningún concepto nuevo, solo usa el mismo conjunto de capas que el bloque encoder de una manera ligeramente diferente."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1623252628887,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"P8SKdOSFA6GU"},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el bloque decoder con la  #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el bloque decoder. Deberías #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","\n","        #trg = [batch size, trg len, hid dim]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        self_trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n","        trg = self.self_attn_layer_norm(trg + self.dropout(self_trg))\n","\n","        #trg = [batch size, trg len, hid dim]\n","            \n","        enc_self_trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n","        trg = self.enc_attn_layer_norm(trg + self.dropout(enc_self_trg))\n","\n","        poswis_trg = self.positionwise_feedforward(trg)\n","        trg = self.ff_layer_norm(trg + self.dropout(poswis_trg))\n","\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return trg, attention"]},{"cell_type":"markdown","metadata":{"id":"ltTk2_v0A6GS"},"source":["### Decoder\n","\n","El objetivo del decoder es tomar la representación codificada de la oración origen, $ Z $, y convertirla en tokens predichos en la oración objetivo, $ \\hat {Y} $. Luego comparamos $ \\hat {Y} $ con los tokens reales en la oración objetivo, $ Y $, para calcular nuestra pérdida, que se usará para calcular los gradientes de nuestros parámetros y luego usamos nuestro optimizador para actualizar nuestros pesos de manera de mejorar nuestras predicciones.\n","\n","![](https://i.imgur.com/g5Armz5.png)\n","\n","El decoder es similar al encoder, sin embargo, tiene dos capas de atención de múltiples cabezales. Una *capa de auto-atención de múltiples cabezales enmascarada* sobre la secuencia objetivo, y una capa de atención de múltiples cabezales que usa la representación del decoder como consulta y la representación del encoder como clave y valor.\n","\n","El decoder utiliza embedding posicionales y los combina, a través de una suma elemento a elemento, con los embeddings escalados de los tokens de destino, seguidos de dropout. Nuevamente, nuestros embeddings posicionales tienen un \"vocabulario\" de 100, lo que significa que pueden aceptar secuencias de hasta 100 tokens de longitud. Esto se puede aumentar si se desea.\n","\n","Los embeddings combinados se pasan a través de $ N $ ***bloques decoder***,definidos en la sección anterior, junto con la frase origen codificada, `enc_src`, y las máscaras de origen y destino. Tenga en cuenta que la cantidad de bloques encoder no tiene que ser igual a la cantidad de bloques decoder, aunque ambas se denoten con $ N $.\n","\n","La representación del decoder después del último bloque se pasa a través de una capa densa, `fc_out`. En PyTorch, la operación softmax está contenida dentro de nuestra función de pérdida, por lo que no necesitamos explícitamente usar una capa softmax aquí.\n","\n","Además de usar la máscara de origen, como hicimos en el encoder para evitar que nuestro modelo preste atención a los tokens `\u003cpad\u003e`, también usamos una máscara de destino. Esto se explicará con más detalle en el modelo \"Seq2Seq\" que encapsula tanto el encoder como el decoder, pero lo esencial es que como estamos procesando todos los tokens de destino a la vez en paralelo, necesitamos un método para evitar que el decoder \"haga trampa\" simplemente \"mirando\" cuál es el siguiente token en la secuencia de destino y emitiéndolo como salida.\n","\n","Nuestro decoder también genera los valores de atención normalizados para que luego podamos graficarlos para ver a qué está prestando atención nuestro modelo."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":273,"status":"ok","timestamp":1623252638762,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"C1jYtpE4A6GU"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, \n","                 device, max_length = 100):\n","        #########################################################################\n","        #TO_DO: Configure las capas que necesita para el decoder con la         #\n","        #       arquitectura definida anteriormente.                            #\n","        #########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","        super().__init__()\n","        self.device = device\n","        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        self.layers = nn.ModuleList([DecoderBlock(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim, \n","                                                  dropout, \n","                                                  device)\n","                                     for _ in range(n_layers)])\n","        self.fc = nn.Linear(hid_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        ########################################################################\n","        # TODO: Implementá la función forward para el decoder. Deberías        #\n","        # usar las capas que definiste en __init__ y especificar la            #\n","        # conectividad de dichas capas                                         #\n","        ########################################################################\n","        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","\n","        #trg = [batch size, trg len]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","                \n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","                            \n","        #pos = [batch size, trg len]\n","            \n","        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n","                \n","        #trg = [batch size, trg len, hid dim]\n","\n","        for layer in self.layers:\n","            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n","\n","        #attention = [batch size, n heads, trg len, src len]\n","\n","        output = self.fc(trg)\n","        \n","        #output = [batch size, trg len, output dim]\n","\n","        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","        ########################################################################\n","        #                          FINAL DE TU CÓDIGO                          #       \n","        ########################################################################\n","\n","        return output, attention"]},{"cell_type":"markdown","metadata":{"id":"-GHVs7BnA6GV"},"source":["### Seq2Seq\n","\n","Finalmente, tenemos el módulo `Seq2Seq` que encapsula el encoder y decoder, además de manejar la creación de las máscaras.\n","\n","La máscara de origen se crea comprobando dónde la secuencia de origen no es igual a un token `\u003cpad\u003e`. Es 1 cuando el token no es un token `\u003cpad\u003e` y 0 cuando lo es. Luego se hace un unsqueeze para que se pueda aplicar correctamente el broadcast al aplicar la máscara a la \"energía\", que tiene forma **_[batch size, n heads, seq len, seq len]_**.\n","\n","La máscara de destino es un poco más complicada. Primero, creamos una máscara para los tokens `\u003cpad\u003e`, como hicimos para la máscara de origen. A continuación, creamos una máscara \"subsecuente\", `trg_sub_mask`, usando` torch.tril`. Esto crea una matriz diagonal donde los elementos por encima de la diagonal serán cero y los elementos por debajo de la diagonal se establecerán en lo que sea que valga el tensor de entrada. En este caso, el tensor de entrada será un tensor lleno de unos. Entonces esto significa que nuestro `trg_sub_mask` se verá así (para un objetivo con 5 tokens):\n","\n","$$\\begin{matrix}\n","1 \u0026 0 \u0026 0 \u0026 0 \u0026 0\\\\\n","1 \u0026 1 \u0026 0 \u0026 0 \u0026 0\\\\\n","1 \u0026 1 \u0026 1 \u0026 0 \u0026 0\\\\\n","1 \u0026 1 \u0026 1 \u0026 1 \u0026 0\\\\\n","1 \u0026 1 \u0026 1 \u0026 1 \u0026 1\\\\\n","\\end{matrix}$$\n","\n","Esto muestra lo que cada token de destino (fila) está autorizado a mirar (columna). El primer token de destino tiene una máscara de **_[1, 0, 0, 0, 0]_**, lo que significa que solo puede mirarse a sí mismo. El segundo token de destino tiene una máscara de **_[1, 1, 0, 0, 0]_**, lo que significa que puede ver el primer y el segundo token de destino.\n","\n","A continuación, a la máscara \"subsecuente\" se le aplica un AND lógico con la máscara de relleno, esto combina las dos máscaras, lo que garantiza que no se puedan atender ni a los tokens posteriores ni a los tokens de relleno. Por ejemplo, si los dos últimos tokens fueran tokens `\u003cpad\u003e`, la máscara se vería así:\n","\n","$$\\begin{matrix}\n","1 \u0026 0 \u0026 0 \u0026 0 \u0026 0\\\\\n","1 \u0026 1 \u0026 0 \u0026 0 \u0026 0\\\\\n","1 \u0026 1 \u0026 1 \u0026 0 \u0026 0\\\\\n","1 \u0026 1 \u0026 1 \u0026 0 \u0026 0\\\\\n","1 \u0026 1 \u0026 1 \u0026 0 \u0026 0\\\\\n","\\end{matrix}$$\n","\n","Una vez creadas las máscaras, se utilizan con el encoder y el decoder junto con las oraciones de origen y destino para obtener nuestra oración de destino predicha, `output`, junto con la atención del decoder sobre la secuencia de origen."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":373,"status":"ok","timestamp":1623252646077,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"IuRrbmDMA6GV"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, \n","                 encoder, \n","                 decoder, \n","                 src_pad_idx, \n","                 trg_pad_idx, \n","                 device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","        \n","    def make_src_mask(self, src):\n","        \n","        #src = [batch size, src len]\n","        \n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","    \n","    def make_trg_mask(self, trg):\n","        \n","        #trg = [batch size, trg len]\n","        \n","        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n","        \n","        #trg_pad_mask = [batch size, 1, 1, trg len]\n","        \n","        trg_len = trg.shape[1]\n","        \n","        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n","        \n","        #trg_sub_mask = [trg len, trg len]\n","            \n","        trg_mask = trg_pad_mask \u0026 trg_sub_mask\n","        \n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        return trg_mask\n","\n","    def forward(self, src, trg):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len]\n","                \n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        \n","        #src_mask = [batch size, 1, 1, src len]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        enc_src = self.encoder(src, src_mask)\n","        \n","        #enc_src = [batch size, src len, hid dim]\n","                \n","        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n","        \n","        #output = [batch size, trg len, output dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return output, attention"]},{"cell_type":"markdown","metadata":{"id":"YPpapCBrA6GW"},"source":["## Entrenamiento del modelo Seq2Seq\n","\n","Ahora podemos definir nuestro encoder y decoder. Este modelo es significativamente más pequeño que los Transformers que se utilizan en investigación hoy en día, pero se puede ejecutar rápidamente en una sola GPU."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1623252653042,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"E2WlzQbOA6GW"},"outputs":[],"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","HID_DIM = 256\n","ENC_LAYERS = 3\n","DEC_LAYERS = 3\n","ENC_HEADS = 8\n","DEC_HEADS = 8\n","ENC_PF_DIM = 512\n","DEC_PF_DIM = 512\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","\n","enc = Encoder(INPUT_DIM, \n","              HID_DIM, \n","              ENC_LAYERS, \n","              ENC_HEADS, \n","              ENC_PF_DIM, \n","              ENC_DROPOUT, \n","              device)\n","\n","dec = Decoder(OUTPUT_DIM, \n","              HID_DIM, \n","              DEC_LAYERS, \n","              DEC_HEADS, \n","              DEC_PF_DIM, \n","              DEC_DROPOUT, \n","              device)"]},{"cell_type":"markdown","metadata":{"id":"_kR-iHeZA6GW"},"source":["Luego, los utilizamos para definir todo nuestro modelo de secuencia a secuencia encapsulado."]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1623252656363,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"DSJbATbDA6GW"},"outputs":[],"source":["SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"]},{"cell_type":"markdown","metadata":{"id":"dBGeDuqqA6GW"},"source":["Podemos verificar el número de parámetros..."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1623252657882,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"3McPPu0LA6GW","outputId":"487a17ef-0452-4dd5-ac74-005bfd63c5dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 9,038,853 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"markdown","metadata":{"id":"w54TbGfXA6GX"},"source":["El documento no menciona qué esquema de inicialización de pesos se usó, sin embargo, Xavier parece ser común entre los modelos de Transformer, así que lo usamos aquí."]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1623252660193,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"9FO86PNJA6GX"},"outputs":[],"source":["def initialize_weights(m):\n","    if hasattr(m, 'weight') and m.weight.dim() \u003e 1:\n","        nn.init.xavier_uniform_(m.weight.data)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":292,"status":"ok","timestamp":1623252661684,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"bpE5QBwLA6GX"},"outputs":[],"source":["model.apply(initialize_weights);"]},{"cell_type":"markdown","metadata":{"id":"UbaglMWaA6GY"},"source":["El optimizador utilizado en el paper original de Transformer utiliza a Adam con una tasa de aprendizaje que tiene un período de \"calentamiento\" y luego un período de \"enfriamiento\". BERT y otros modelos de Transformer usan Adam con una tasa de aprendizaje fija, así que lo vamos a implementar así. Consulte  [este](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer) enlace para obtener más detalles sobre la programación de la tasa de aprendizaje del Transformer original.\n","\n","Tenga en cuenta que la tasa de aprendizaje debe ser más baja que la predeterminada utilizada por Adam o, de lo contrario, el aprendizaje es inestable."]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":313,"status":"ok","timestamp":1623252663873,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"Lruyu6kNA6GY"},"outputs":[],"source":["LEARNING_RATE = 0.0005\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"CA4cYzRHA6GY"},"source":["A continuación, definimos nuestra función de pérdida, asegurándonos de ignorar las pérdidas calculadas sobre los tokens `\u003cpad\u003e`."]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1623252665341,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"Ge2EpK6HA6GY"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"]},{"cell_type":"markdown","metadata":{"id":"gjE6_AqrA6GZ"},"source":["Luego, vamos a definir nuestro ciclo de entrenamiento. \n","\n","Como queremos que nuestro modelo prediga el token `\u003ceos\u003e` pero que no sea una entrada en nuestro modelo, simplemente cortamos el token `\u003ceos\u003e` del final de la secuencia. Por lo tanto:\n","\n","$$\\begin{align*}\n","\\text{trg} \u0026= [sos, x_1, x_2, x_3, eos]\\\\\n","\\text{trg[:-1]} \u0026= [sos, x_1, x_2, x_3]\n","\\end{align*}$$\n","\n","$ x_i $ denota el elemento real de la secuencia objetivo. Luego introducimos esto en el modelo para obtener una secuencia predicha que, con suerte, debería predecir el token `\u003ceos\u003e`:\n","\n","$$\\begin{align*}\n","\\text{output} \u0026= [y_1, y_2, y_3, eos]\n","\\end{align*}$$\n","\n","$ y_i $ denota el elemento de secuencia objetivo predicho. Luego calculamos nuestra pérdida usando el tensor `trg` original con la ficha` \u003csos\u003e ` eliminada del frente, dejando la ficha` \u003ceos\u003e `:\n","\n","$$\\begin{align*}\n","\\text{output} \u0026= [y_1, y_2, y_3, eos]\\\\\n","\\text{trg[1:]} \u0026= [x_1, x_2, x_3, eos]\n","\\end{align*}$$\n","\n","Luego calculamos nuestras pérdidas y actualizamos nuestros parámetros como es estándar."]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":391,"status":"ok","timestamp":1623252668350,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"rErUo2hfA6GZ"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output, _ = model(src, trg[:,:-1])\n","                \n","        #output = [batch size, trg len - 1, output dim]\n","        #trg = [batch size, trg len]\n","            \n","        output_dim = output.shape[-1]\n","            \n","        output = output.contiguous().view(-1, output_dim)\n","        trg = trg[:,1:].contiguous().view(-1)\n","                \n","        #output = [batch size * trg len - 1, output dim]\n","        #trg = [batch size * trg len - 1]\n","            \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"-FlhnAZIA6GZ"},"source":["El ciclo de evaluación es el mismo que el ciclo de entrenamiento, solo que sin los cálculos de gradiente y las actualizaciones de parámetros."]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1623252671225,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"HlTv3eYaA6GZ"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output, _ = model(src, trg[:,:-1])\n","            \n","            #output = [batch size, trg len - 1, output dim]\n","            #trg = [batch size, trg len]\n","            \n","            output_dim = output.shape[-1]\n","            \n","            output = output.contiguous().view(-1, output_dim)\n","            trg = trg[:,1:].contiguous().view(-1)\n","            \n","            #output = [batch size * trg len - 1, output dim]\n","            #trg = [batch size * trg len - 1]\n","            \n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"VtwlY6t-A6Ga"},"source":["Luego definimos una pequeña función que podemos usar para decirnos cuánto tarda una época."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1623252674108,"user":{"displayName":"Exequiel Santucho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1tgKvLHIAEKKcLFWud1Mj1s1pncyPIR5P8RA8fg=s64","userId":"10709628178102361168"},"user_tz":180},"id":"Zu2ZVoOlA6Gb"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"pdSyPhFQA6Gb"},"source":["Finalmente, entrenamos nuestro modelo. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LInHGoULA6Gb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 01 | Time: 11m 35s\n","\tTrain Loss: 1.597 | Train PPL:   4.936\n","\t Val. Loss: 1.701 |  Val. PPL:   5.478\n","Epoch: 02 | Time: 11m 26s\n","\tTrain Loss: 1.417 | Train PPL:   4.125\n","\t Val. Loss: 1.652 |  Val. PPL:   5.218\n","Epoch: 03 | Time: 11m 29s\n","\tTrain Loss: 1.274 | Train PPL:   3.575\n","\t Val. Loss: 1.632 |  Val. PPL:   5.116\n","Epoch: 04 | Time: 11m 24s\n","\tTrain Loss: 1.151 | Train PPL:   3.162\n","\t Val. Loss: 1.628 |  Val. PPL:   5.093\n","Epoch: 05 | Time: 11m 33s\n","\tTrain Loss: 1.050 | Train PPL:   2.859\n","\t Val. Loss: 1.605 |  Val. PPL:   4.977\n"]}],"source":["N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss \u003c best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut6-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"]},{"cell_type":"markdown","metadata":{"id":"5otdslonA6Gc"},"source":["Cargamos nuestros \"mejores\" parámetros y logramos lograr una mejor perplejidad de prueba que todos los modelos que probamos en el modelo anterior."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0ONyrtpA6Gd"},"outputs":[],"source":["model.load_state_dict(torch.load('tut6-model.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"]},{"cell_type":"markdown","metadata":{"id":"UYbtH6AJA6Gd"},"source":["## Inferencia\n","\n","Ahora con nuestro modelo podemos realizar traducciones con la función `translate_sentence` a continuación.\n","\n","Los pasos dados son:\n","- tokenizar la oración origen si no se ha tokenizado \n","- añadir los tokens `\u003csos\u003e` y `\u003ceos\u003e`\n","- numericalizar la oración fuente (asignarle un entero a cada token)\n","- convertir en un tensor y agregar una dimensión de lote\n","- crear la máscara de la oración fuente\n","- alimentar la frase origen y la máscara en el encoder\n","- crear una lista para contener la oración de salida, inicializada con un token `\u003csos\u003e`\n","- mientras no hayamos alcanzado una longitud máxima\n","  - convertir la predicción actual de la oración de salida en un tensor con una dimensión de lote\n","  - crear una máscara de oración de destino\n","  - colocar la salida actual, la salida del encoder y ambas máscaras en el decoder\n","  - obtener la próxima predicción del token de salida del decoder junto con la atención\n","  - agregar predicción a la lista de predicciones de la oración de salida actual\n","  - meter un break si la predicción fue un token `\u003ceos\u003e`\n","- convertir la oración de salida de índices a tokens\n","- devolver la oración de salida (con el token `\u003csos\u003e` eliminado) y la atención de la última capa"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdsX6btTA6Gd"},"outputs":[],"source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n","    #########################################################################\n","    #TO_DO: Siga los pasos anteriores para implementear la función que      #\n","    #       traduce una oración del alemán al inglés                        #\n","    #########################################################################\n","    # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n","\n","    model.eval()\n","        \n","    if isinstance(sentence, str):\n","        nlp = spacy.load('de_core_news_sm')\n","        tokens = [token.text.lower() for token in nlp(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n","    src_mask = model.make_src_mask(src_tensor)\n","    \n","    with torch.no_grad():\n","        enc_src = model.encoder(src_tensor, src_mask)\n","\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","    for i in range(max_len):\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n","        trg_mask = model.make_trg_mask(trg_tensor)\n","\n","        with torch.no_grad():\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n","        \n","        pred_token = output.argmax(2)[:,-1].item()\n","        trg_indexes.append(pred_token)\n","\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","    \n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","\n","    # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n","    ########################################################################\n","    #                          FINAL DE TU CÓDIGO                          #       \n","    ########################################################################\n","    \n","    return trg_tokens[1:], attention"]},{"cell_type":"markdown","metadata":{"id":"DUAFtHzjA6Ge"},"source":["Ahora definiremos una función que muestra la atención sobre la oración fuente para cada paso de la decodificación. Como este modelo tiene 8 cabezales podemos ver la atención para cada una de ellos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JuBWFzL3A6Ge"},"outputs":[],"source":["def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n","    \n","    assert n_rows * n_cols == n_heads\n","    \n","    fig = plt.figure(figsize=(15,25))\n","    \n","    for i in range(n_heads):\n","        \n","        ax = fig.add_subplot(n_rows, n_cols, i+1)\n","        \n","        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n","\n","        cax = ax.matshow(_attention, cmap='bone')\n","\n","        ax.tick_params(labelsize=12)\n","        ax.set_xticklabels(['']+['\u003csos\u003e']+[t.lower() for t in sentence]+['\u003ceos\u003e'], \n","                           rotation=45)\n","        ax.set_yticklabels(['']+translation)\n","\n","        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","    plt.show()\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"-5YT-Q3FA6Ge"},"source":["Primero, obtendremos un ejemplo del conjunto de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xt1lVGr-A6Ge"},"outputs":[],"source":["example_idx = 8\n","\n","src = vars(train_data.examples[example_idx])['src']\n","trg = vars(train_data.examples[example_idx])['trg']\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')"]},{"cell_type":"markdown","metadata":{"id":"ZFIo-PIuA6Ge"},"source":["Nuestra traducción se ve bastante bien, aunque nuestro modelo cambia *is walking by* por *walks by*. El significado sigue siendo el mismo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLRw8FVOA6Gf"},"outputs":[],"source":["translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","\n","print(f'predicted trg = {translation}')"]},{"cell_type":"markdown","metadata":{"id":"vgzWSchUA6Gf"},"source":["Podemos ver la atención de cada cabezal a continuación. Cada uno es ciertamente diferente, pero es difícil (quizás imposible) razonar sobre a qué ha aprendido realmente a prestar atención. Algunos cabezales prestan toda su atención a \"eine\" cuando traducen \"a\", otros no lo hacen en absoluto y otros lo hacen un poco. Todos parecen seguir el patrón similar de \"escalera descendente\" y la atención al sacar las dos últimas fichas se distribuye por igual entre las dos últimas fichas de la oración de entrada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_VBMcJaA6Gf"},"outputs":[],"source":["display_attention(src, translation, attention)"]},{"cell_type":"markdown","metadata":{"id":"sqpyN8iJA6Gg"},"source":["A continuación, obtengamos un ejemplo con el que no se ha entrenado el modelo desde el conjunto de validación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqXAsupHA6Gg"},"outputs":[],"source":["example_idx = 6\n","\n","src = vars(valid_data.examples[example_idx])['src']\n","trg = vars(valid_data.examples[example_idx])['trg']\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')"]},{"cell_type":"markdown","metadata":{"id":"2gQiVLjqA6Gg"},"source":["El modelo lo traduce cambiando \"*is running*\" a simplemente \"*runs*\", pero es un intercambio aceptable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnhuaDrsA6Gh"},"outputs":[],"source":["translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","\n","print(f'predicted trg = {translation}')"]},{"cell_type":"markdown","metadata":{"id":"4LJ0gGPAA6Gh"},"source":["Una vez más, algunos cabezales prestan total atención a \"ein\" mientras que otros no le prestan atención. Una vez más, la mayoría de los cabezales parecen extender su atención tanto al punto como a los tokens `\u003ceos\u003e` en la oración de origen cuando generan el punto y el token `\u003ceos\u003e` en la oración objetivo predicha, aunque algunos parecen prestar atención a tokens cercanos del comienzo de la oración."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYJuvrTqA6Gh"},"outputs":[],"source":["display_attention(src, translation, attention)"]},{"cell_type":"markdown","metadata":{"id":"enVxd6mNA6Gh"},"source":["Finalmente, veremos un ejemplo de los datos de prueba."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZTAn6cDA6Gh"},"outputs":[],"source":["example_idx = 10\n","\n","src = vars(test_data.examples[example_idx])['src']\n","trg = vars(test_data.examples[example_idx])['trg']\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')"]},{"cell_type":"markdown","metadata":{"id":"AaWdg89NA6Gh"},"source":["¡Una traducción perfecta!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWpkD9LBA6Gi"},"outputs":[],"source":["translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","\n","print(f'predicted trg = {translation}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2xftlpVA6Gj"},"outputs":[],"source":["display_attention(src, translation, attention)"]},{"cell_type":"markdown","metadata":{"id":"PgPigiTVA6Gj"},"source":["## BLEU\n","\n","Finalmente calculamos el BLEU para el Transformer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEclB7RSA6Gj"},"outputs":[],"source":["from torchtext.data.metrics import bleu_score\n","\n","def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n","    \n","    trgs = []\n","    pred_trgs = []\n","    \n","    for datum in data:\n","        \n","        src = vars(datum)['src']\n","        trg = vars(datum)['trg']\n","        \n","        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n","        \n","        #cut off \u003ceos\u003e token\n","        pred_trg = pred_trg[:-1]\n","        \n","        pred_trgs.append(pred_trg)\n","        trgs.append([trg])\n","        \n","    return bleu_score(pred_trgs, trgs)"]},{"cell_type":"markdown","metadata":{"id":"PVc5cFJBA6Gj"},"source":["Obtenemos una puntuación BLEU de 36,52, que supera el  ~ 28 del modelo RNN basado en la atención. ¡Todo esto con la menor cantidad de parámetros y el tiempo de entrenamiento más rápido!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yf95cVmgA6Gk"},"outputs":[],"source":["bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n","\n","print(f'BLEU score = {bleu_score*100:.2f}')"]}],"metadata":{"colab":{"name":"Trabajo práctico 8-ExequielSantucho","provenance":[{"file_id":"1uQtT3RPK6rspkC8V58fNPnlAkkX8j7th","timestamp":1623162106066}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}